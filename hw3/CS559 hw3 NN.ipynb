{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    # draw data\n",
    "\n",
    "\n",
    "def shuffle():\n",
    "    # shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# TODO get x(data) and y(label) from iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffle(x, y)\n",
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split data into train and test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:  (120, 4)\n",
      "train label shape:  (120, 1)\n",
      "test data shape:  (30, 1)\n",
      "test label shape:  (30, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: \", train_x.shape)\n",
    "print(\"train label shape: \", train_y.shape)\n",
    "print(\"test data shape: \", test_y.shape)\n",
    "print(\"test label shape: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    # normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = normalize(train_x)\n",
    "test_x = normalize(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of train data:  [ 7.99360578e-16  3.83211981e-15  6.87413089e-16 -1.85037171e-16]\n",
      "std of train data:  [1. 1. 1. 1.]\n",
      "mean of test data:  [-1.17498603e-15  5.47710025e-16  1.59131967e-16 -5.44009282e-16]\n",
      "std of test data:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"mean of train data: \", train_x.mean(axis=0))\n",
    "print(\"std of train data: \", train_x.std(axis=0))\n",
    "print(\"mean of test data: \", test_x.mean(axis=0))\n",
    "print(\"std of test data: \", test_x.std(axis=0))\n",
    "\n",
    "# you can use different normalization strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [4, 10, 3]\n",
    "# define your network \n",
    "# here is  (num_of_data, 4) -> (num_of_data, 10) -> (num_of_data, num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thetas_init(layers):\n",
    "    # initialize thetas\n",
    "\n",
    "def bias_init(layers):\n",
    "    # initialize biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta shape:  (4, 10)\n",
      "theta shape:  (10, 3)\n",
      "bia shape:  (10,)\n",
      "bia shape:  (3,)\n"
     ]
    }
   ],
   "source": [
    "thetas = thetas_init(layers)\n",
    "biases = bias_init(layers)\n",
    "for theta in thetas.values():\n",
    "    print(\"theta shape: \", np.shape(theta))\n",
    "for bias in biases.values():\n",
    "    print(\"bia shape: \", np.shape(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid():\n",
    "    # activate function\n",
    "\n",
    "def feedforward_propagation():\n",
    "    # x -> logits\n",
    "\n",
    "def loss_func():\n",
    "    # loss between pred_logit and true_label(labels should be one-hot encoding, for example, \n",
    "    # class 2 should be [0,0,1])\n",
    "\n",
    "def gradient():\n",
    "    # compute gradients based on chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = None\n",
    "lr = None\n",
    "# define your learning rate and number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.0811503526034505, Trainning Accuracy: 34.166666666666664\n",
      "Training loss: 2.0523569330257367, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 2.028379848328214, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 2.008403316450711, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9917482069495132, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9778507001988406, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9662435086504686, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9565397021682405, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9484189853722578, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9416161795027338, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9359116274655104, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9311232426531948, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9270999429457603, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9237162404816117, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9208677891853336, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9184677222655762, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.916443639305846, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9147351265320514, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9132917142939168, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9120711929772136, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9110382228350256, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9101631849909864, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9094212305154428, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9087914923603349, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.908256431368736, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9078012928127495, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9074136541781275, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9070830483868866, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9068006494789345, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9065590100814767, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9063518418791807, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9061738318384873, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9060204882006053, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9058880112918704, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9057731850493855, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.90567328585852, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9055860058743932, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9055093884745042, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9054417738822782, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.905381753326295, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9053281303694736, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9052798882661808, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9052361623912801, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9051962169400312, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9051594252268556, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9051252530187788, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.905093244429438, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9050630099748849, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9050342164555607, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9050065783817014, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9049798507038744, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9049538226476186, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9049283124825365, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9049031630825581, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904878238156326, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9048534190453428, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9048286020033287, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9048036958835448, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904778620172067, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9047533033145045, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9047276812916785, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9047016964065469, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9046752962504228, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9046484328213715, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9046210617717934, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9045931417656787, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9045646339289704, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9045355013789773, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9045057088208885, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9044752222012544, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9044440084098044, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904412035022285, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904379270078086, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9043456818873652, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904311238863163, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9042759093746766, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.90423966161844, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9042024635046275, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.90416428255612, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904125085818323, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.90408483977802, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904043510289797, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.904001062508794, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.903957460828721, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.903912668824221, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9038666491968106, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9038193637237355, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9037707732091629, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9037208374372325, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9036695151265433, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9036167638857142, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.903562540169714, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9035067992366939, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9034494951050773, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9033905805107347, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9033300068640362, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.903267724206656, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.90320368116798, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9031378249210118, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9030701011376603, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9030004539433247, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9029288258707018, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9028551578127246, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9027793889745859, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9027014568247689, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9026212970450458, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.902538843479383, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9024540280817097, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9023667808625122, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9022770298342053, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9021847009552488, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9020897180729714, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.901992002865073, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9018914747797662, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9017880509745344, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.901681646253477, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9015721730032098, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9014595411273032, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9013436579792273, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.901224428293787, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9011017541170199, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9009755347345465, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.900845666598345, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9007120432519382, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9005745552539803, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9004330901002282, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9002875321438835, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.9001377625143019, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8999836590340593, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8998250961343688, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8996619447688525, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8994940723256633, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.899321342537959, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.899143615392745, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8989607470380852, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8987725896886967, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.898578991529953, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8983797966203102, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8981748447921831, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8979639715513035, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8977470079746008, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8975237806066325, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8972941113546324, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8970578173822068, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8968147110017555, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8965645995656757, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8963072853564242, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8960425654755184, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8957702317315688, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.895490070527435, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8952018627466203, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8949053836390135, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8946004027061147, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.89428668358587, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8939639839372795, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8936320553249235, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8932906431035956, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8929394863032158, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8925783175142297, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.892206862773712, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.891824841452392, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8914319661428574, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8910279425491865, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8906124693782984, Trainning Accuracy: 35.833333333333336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8901852382333038, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8897459335091749, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8892942322910704, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8888298042556582, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.888352311575814, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8878614088290826, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8873567429103195, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8868379529489425, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.886304670231255, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8857565181283185, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8851931120298735, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8846140592848448, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8840189591489713, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8834074027401384, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8827789730020161, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8821332446766164, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.881469784286427, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8807881501267842, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8800878922691875, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8793685525762682, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8786296647291585, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.87787075426802, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.877091338646526, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8762909273010873, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8754690217356635, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8746251156229894, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8737586949230776, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8728692380198717, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8719562158769263, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8710190922130139, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.870057323698548, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8690703601737335, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8680576448893342, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8670186147709658, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.865952700707792, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8648593278665109, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8637379160314806, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8625878799718296, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8614086298363521, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.860199571576972, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.858960107401502, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8576896362563953, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8563875543401156, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8550532556477108, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.853686132547088, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8522855763874273, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.850850978140081, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8493817290722119, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8478772214533288, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8463368492947656, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.844760009122035, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8431461007798624, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8414945282695623, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.839804700618302, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8380760327796106, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8363079465643684, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8344998716013228, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8326512463260107, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8307615189968, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8288301487365521, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8268566065982492, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8248403766527141, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.822780957096363, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8206778613767327, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.81853061933333, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8163387783511407, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8141019045239553, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8118195838244675, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8094914232779191, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8071170521358808, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.8046961230465983, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.802228313218161, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7997133255706084, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7971508898729647, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7945407638610533, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7918827343318666, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7891766182101714, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.786422263582978, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7836195506974584, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7807683929178988, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7778687376372668, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7749205671390342, Trainning Accuracy: 35.833333333333336\n",
      "Training loss: 1.7719238994049442, Trainning Accuracy: 36.666666666666664\n",
      "Training loss: 1.7688787888645203, Trainning Accuracy: 36.666666666666664\n",
      "Training loss: 1.7657853270822321, Trainning Accuracy: 36.666666666666664\n",
      "Training loss: 1.762643643378387, Trainning Accuracy: 37.5\n",
      "Training loss: 1.7594539053799985, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.756216319498088, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.752931131328125, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.749598625970557, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.7462191282686896, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.7427930029614804, Trainning Accuracy: 38.333333333333336\n",
      "Training loss: 1.7393206547491424, Trainning Accuracy: 39.166666666666664\n",
      "Training loss: 1.7358025282698295, Trainning Accuracy: 39.166666666666664\n",
      "Training loss: 1.7322391079860238, Trainning Accuracy: 39.166666666666664\n",
      "Training loss: 1.7286309179796588, Trainning Accuracy: 39.166666666666664\n",
      "Training loss: 1.7249785216554008, Trainning Accuracy: 40.0\n",
      "Training loss: 1.7212825213519323, Trainning Accuracy: 40.0\n",
      "Training loss: 1.7175435578614946, Trainning Accuracy: 40.0\n",
      "Training loss: 1.713762309858375, Trainning Accuracy: 41.66666666666667\n",
      "Training loss: 1.7099394932374423, Trainning Accuracy: 41.66666666666667\n",
      "Training loss: 1.7060758603642625, Trainning Accuracy: 42.5\n",
      "Training loss: 1.7021721992387324, Trainning Accuracy: 42.5\n",
      "Training loss: 1.6982293325745768, Trainning Accuracy: 43.333333333333336\n",
      "Training loss: 1.6942481167974501, Trainning Accuracy: 43.333333333333336\n",
      "Training loss: 1.6902294409647567, Trainning Accuracy: 45.83333333333333\n",
      "Training loss: 1.6861742256106536, Trainning Accuracy: 45.83333333333333\n",
      "Training loss: 1.6820834215200537, Trainning Accuracy: 45.83333333333333\n",
      "Training loss: 1.677958008435739, Trainning Accuracy: 45.83333333333333\n",
      "Training loss: 1.6737989937029918, Trainning Accuracy: 48.333333333333336\n",
      "Training loss: 1.669607410856403, Trainning Accuracy: 48.333333333333336\n",
      "Training loss: 1.6653843181537482, Trainning Accuracy: 50.83333333333333\n",
      "Training loss: 1.6611307970620142, Trainning Accuracy: 50.83333333333333\n",
      "Training loss: 1.6568479507008225, Trainning Accuracy: 51.66666666666667\n",
      "Training loss: 1.6525369022486274, Trainning Accuracy: 52.5\n",
      "Training loss: 1.6481987933171682, Trainning Accuracy: 52.5\n",
      "Training loss: 1.643834782299703, Trainning Accuracy: 52.5\n",
      "Training loss: 1.639446042698601, Trainning Accuracy: 52.5\n",
      "Training loss: 1.6350337614378503, Trainning Accuracy: 53.333333333333336\n",
      "Training loss: 1.6305991371660116, Trainning Accuracy: 54.166666666666664\n",
      "Training loss: 1.6261433785550792, Trainning Accuracy: 54.166666666666664\n",
      "Training loss: 1.621667702600617, Trainning Accuracy: 55.00000000000001\n",
      "Training loss: 1.617173332928404, Trainning Accuracy: 56.666666666666664\n",
      "Training loss: 1.6126614981126928, Trainning Accuracy: 58.333333333333336\n",
      "Training loss: 1.6081334300109862, Trainning Accuracy: 59.166666666666664\n",
      "Training loss: 1.6035903621200585, Trainning Accuracy: 59.166666666666664\n",
      "Training loss: 1.5990335279577248, Trainning Accuracy: 60.0\n",
      "Training loss: 1.5944641594746263, Trainning Accuracy: 60.0\n",
      "Training loss: 1.589883485500048, Trainning Accuracy: 60.0\n",
      "Training loss: 1.5852927302255333, Trainning Accuracy: 60.0\n",
      "Training loss: 1.5806931117297718, Trainning Accuracy: 60.0\n",
      "Training loss: 1.5760858405479645, Trainning Accuracy: 61.66666666666667\n",
      "Training loss: 1.5714721182885842, Trainning Accuracy: 62.5\n",
      "Training loss: 1.566853136300151, Trainning Accuracy: 65.0\n",
      "Training loss: 1.562230074390354, Trainning Accuracy: 65.0\n",
      "Training loss: 1.5576040995995606, Trainning Accuracy: 65.0\n",
      "Training loss: 1.5529763650304598, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5483480087353068, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.543720152661945, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.539093901659527, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5344703425445698, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5298505432277503, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5252355519015832, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5206263962888993, Trainning Accuracy: 65.83333333333333\n",
      "Training loss: 1.5160240829518326, Trainning Accuracy: 66.66666666666666\n",
      "Training loss: 1.5114295966608013, Trainning Accuracy: 66.66666666666666\n",
      "Training loss: 1.5068438998227929, Trainning Accuracy: 66.66666666666666\n",
      "Training loss: 1.502267931968068, Trainning Accuracy: 66.66666666666666\n",
      "Training loss: 1.4977026092942547, Trainning Accuracy: 66.66666666666666\n",
      "Training loss: 1.4931488242666298, Trainning Accuracy: 67.5\n",
      "Training loss: 1.488607445273267, Trainning Accuracy: 67.5\n",
      "Training loss: 1.4840793163336028, Trainning Accuracy: 67.5\n",
      "Training loss: 1.4795652568588569, Trainning Accuracy: 68.33333333333333\n",
      "Training loss: 1.4750660614626598, Trainning Accuracy: 68.33333333333333\n",
      "Training loss: 1.470582499820145, Trainning Accuracy: 68.33333333333333\n",
      "Training loss: 1.4661153165737062, Trainning Accuracy: 68.33333333333333\n",
      "Training loss: 1.461665231283552, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4572329384211518, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4528191074036216, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.448424382667092, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4440493837770574, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4396947055737335, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4353609183504183, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.431048568062891, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4267581765678812, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.422490241888677, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4182452385059592, Trainning Accuracy: 69.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.4140236176720036, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.409825807746404, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4056522145515469, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.4015032217460879, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3973791912147528, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3932804634728233, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3892073580837419, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.385160174088308, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3811391904440147, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3771446664731277, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3731768423181658, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3692359394035216, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3653221609019903, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3614356922050757, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3575767013959696, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3537453397241817, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3499417420808493, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3461660274738132, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3424182995016058, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3386986468255566, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3350071436392676, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3313438501347652, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.327708812964698, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3241020656999802, Trainning Accuracy: 69.16666666666667\n",
      "Training loss: 1.3205236292823437, Trainning Accuracy: 70.0\n",
      "Training loss: 1.3169735124712991, Trainning Accuracy: 70.0\n",
      "Training loss: 1.313451712285061, Trainning Accuracy: 70.0\n",
      "Training loss: 1.3099582144350146, Trainning Accuracy: 70.0\n",
      "Training loss: 1.3064929937533671, Trainning Accuracy: 70.0\n",
      "Training loss: 1.3030560146136456, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2996472313437488, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2962665886312947, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2929140219210387, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2895894578041638, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2862928143992847, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2830240017250207, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2797829220640355, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2765694703184498, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2733835343565711, Trainning Accuracy: 70.0\n",
      "Training loss: 1.270224995350894, Trainning Accuracy: 70.0\n",
      "Training loss: 1.267093728107354, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2639896013858305, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2609124782119143, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2578622161799722, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2548386677475543, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2518416805212023, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2488710975337347, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2459267575130895, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2430084951428189, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2401161413143436, Trainning Accuracy: 70.0\n",
      "Training loss: 1.237249523371072, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2344084653445115, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2315927881824942, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2288023099696492, Trainning Accuracy: 70.0\n",
      "Training loss: 1.226036846140264, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2232962096836686, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2205802113423034, Trainning Accuracy: 70.0\n",
      "Training loss: 1.217888659802603, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2152213618788619, Trainning Accuracy: 70.0\n",
      "Training loss: 1.2125781226902332, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.209958745831012, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.2073630335343666, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.2047907868296779, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.2022418056936377, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.1997158891952713, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.1972128356350453, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.1947324426782102, Trainning Accuracy: 70.83333333333334\n",
      "Training loss: 1.1922745074825456, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1898388268206552, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1874251971969727, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1850334149596304, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1826632764073401, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1803145778914386, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.177987115913244, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1756806872168706, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1733950888776392, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1711301183862337, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1688855737287331, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.166661253462656, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.164456956789156, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.162272483621493, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1601076346499053, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.157962211403016, Trainning Accuracy: 71.66666666666667\n",
      "Training loss: 1.1558360163058863, Trainning Accuracy: 73.33333333333333\n",
      "Training loss: 1.1537288527348384, Trainning Accuracy: 74.16666666666667\n",
      "Training loss: 1.1516405250691661, Trainning Accuracy: 74.16666666666667\n",
      "Training loss: 1.149570838739839, Trainning Accuracy: 74.16666666666667\n",
      "Training loss: 1.1475196002753192, Trainning Accuracy: 74.16666666666667\n",
      "Training loss: 1.1454866173445912, Trainning Accuracy: 75.0\n",
      "Training loss: 1.1434716987975138, Trainning Accuracy: 75.0\n",
      "Training loss: 1.1414746547025902, Trainning Accuracy: 75.0\n",
      "Training loss: 1.1394952963822622, Trainning Accuracy: 75.0\n",
      "Training loss: 1.1375334364458187, Trainning Accuracy: 75.0\n",
      "Training loss: 1.1355888888200132, Trainning Accuracy: 75.83333333333333\n",
      "Training loss: 1.1336614687774842, Trainning Accuracy: 75.83333333333333\n",
      "Training loss: 1.1317509929630605, Trainning Accuracy: 75.83333333333333\n",
      "Training loss: 1.1298572794180415, Trainning Accuracy: 75.83333333333333\n",
      "Training loss: 1.1279801476025335, Trainning Accuracy: 75.83333333333333\n",
      "Training loss: 1.126119418415921, Trainning Accuracy: 76.66666666666667\n",
      "Training loss: 1.124274914215552, Trainning Accuracy: 76.66666666666667\n",
      "Training loss: 1.1224464588337124, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1206338775929616, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1188369973198988, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1170556463574308, Trainning Accuracy: 77.5\n",
      "Training loss: 1.115289654575605, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1135388533810702, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1118030757252302, Trainning Accuracy: 77.5\n",
      "Training loss: 1.110082156111146, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1083759305992458, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1066842368118972, Trainning Accuracy: 77.5\n",
      "Training loss: 1.1050069139368963, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.1033438027299232, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.1016947455160149, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.1000595861901032, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.0984381702166643, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.096830344628522, Trainning Accuracy: 78.33333333333333\n",
      "Training loss: 1.0952359580248499, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.093654860568413, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0920869039820893, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0905319415447057, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0889898280862287, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.087460419982346, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0859435751484667, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.084439153033181, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0829470146112015, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.081467022375828, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.0799990403309498, Trainning Accuracy: 79.16666666666666\n",
      "Training loss: 1.078542933982627, Trainning Accuracy: 80.0\n",
      "Training loss: 1.0770985703302676, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0756658178574319, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0742445465222847, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0728346277477219, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.071435934411189, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0700483408342212, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0686717227717157, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0673059574009647, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0659509233104598, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0646065004884955, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0632725703115784, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0619490155326667, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0606357202692522, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0593325699912988, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0580394515090554, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0567562529607535, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0554828638002052, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0542191747843097, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.052965077960487, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0517204666540434, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.050485235455483, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0492592802077767, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0480424979935918, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0468347871225023, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0456360471181774, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.044446178705564, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0432650837980686, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0420926654847453, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0409288280174995, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0397734767983104, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0386265183664833, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0374878603859323, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0363574116325043, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0352350819813458, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0341207823943204, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0330144249074777, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0319159226185854, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0308251896747191, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0297421412599226, Trainning Accuracy: 80.83333333333333\n",
      "Training loss: 1.0286666935829356, Trainning Accuracy: 80.83333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0275987638649997, Trainning Accuracy: 81.66666666666667\n",
      "Training loss: 1.0265382703277355, Trainning Accuracy: 81.66666666666667\n",
      "Training loss: 1.025485132181106, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0244392696114588, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0234006037696546, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0223690567592858, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0213445516249828, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0203270123408152, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0193163637987834, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0183125317974118, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0173154430304328, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0163250250755769, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0153412063834577, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0143639162665599, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0133930848883328, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0124286432523826, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0114705231917736, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0105186573584295, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0095729792126467, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0086334230127074, Trainning Accuracy: 82.5\n",
      "Training loss: 1.007699923804605, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0067724174118708, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0058508404255144, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0049351301940659, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0040252248137282, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0031210631186371, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0022225846712285, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0013297297527122, Trainning Accuracy: 82.5\n",
      "Training loss: 1.0004424393536542, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9995606551646657, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9986843195671985, Trainning Accuracy: 82.5\n",
      "Training loss: 0.997813375624447, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9969477670723572, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9960874383107391, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9952323343944873, Trainning Accuracy: 82.5\n",
      "Training loss: 0.994382401024904, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9935375845411274, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9926978319116633, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9918630907260189, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9910333091864413, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9902084360997554, Trainning Accuracy: 82.5\n",
      "Training loss: 0.989388420869304, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9885732134869869, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9877627645254018, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9869570251300827, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9861559470118353, Trainning Accuracy: 82.5\n",
      "Training loss: 0.985359482439172, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9845675842308415, Trainning Accuracy: 82.5\n",
      "Training loss: 0.983780205748454, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9829973008892038, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9822188240786806, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9814447302637802, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9806749749057013, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9799095139730385, Trainning Accuracy: 82.5\n",
      "Training loss: 0.9791483039349629, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9783913017544913, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9776384648818482, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9768897512479112, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9761451192577436, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9754045277842162, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9746679361617099, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.973935304179905, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9732065920776516, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9724817605369235, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9717607706768532, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9710435840478455, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9703301626257731, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.969620468806248, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.968914465398972, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9682121156221631, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9675133830970571, Trainning Accuracy: 83.33333333333334\n",
      "Training loss: 0.9668182318424862, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9661266262695268, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9654385311762259, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9647539117423946, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9640727335244755, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9633949624504796, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9627205648149925, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9620495072742477, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9613817568412706, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.960717280881086, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9600560471059926, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.959398023570904, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9587431786687503, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9580914811259466, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9574428999979219, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9567974046647101, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9561549648266016, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9555155504998558, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9548791320124711, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9542456800000151, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9536151654015107, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9529875594553807, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9523628336954474, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.9517409599469865, Trainning Accuracy: 84.16666666666667\n",
      "Training loss: 0.951121910322838, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9505056572195695, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9498921733136916, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9492814315579273, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9486734051775317, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9480680676666636, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9474653927848062, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9468653545532376, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9462679272515511, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9456730854142215, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9450808038272207, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9444910575246789, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9439038217855925, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9433190721305768, Trainning Accuracy: 85.0\n",
      "Training loss: 0.942736784318664, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9421569343441455, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9415794984334566, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9410044530421056, Trainning Accuracy: 85.0\n",
      "Training loss: 0.940431774851644, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9398614407666791, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9392934279119274, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9387277136293086, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9381642754750791, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9376030912170054, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9370441388315766, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9364873965012542, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9359328426117599, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9353804557494012, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9348302146984325, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9342820984384533, Trainning Accuracy: 85.0\n",
      "Training loss: 0.933736086141842, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9331921571712234, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9326502910769725, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9321104675947511, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9315726666430791, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9310368683209376, Trainning Accuracy: 85.0\n",
      "Training loss: 0.930503052905405, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9299712008493272, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9294412927790155, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9289133094919803, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9283872319546912, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9278630413003717, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9273407188268201, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9268202459942625, Trainning Accuracy: 85.0\n",
      "Training loss: 0.926301604423233, Trainning Accuracy: 85.0\n",
      "Training loss: 0.9257847758924844, Trainning Accuracy: 85.0\n",
      "Training loss: 0.925269742336925, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9247564858455856, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9242449886596117, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9237352331702843, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9232272019170665, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9227208775856769, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9222162430061887, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9217132811511546, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9212119751337559, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9207123082059772, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.920214263756807, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9197178253104591, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9192229765246198, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9187297011887199, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9182379832222265, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9177478066729602, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9172591557154343, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9167720146492147, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9162863678973039, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.915802200004545, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9153194956360468, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9148382395756313, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9143584167243, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9138800120987227, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9134030108297443, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9129273981609124, Trainning Accuracy: 85.83333333333333\n",
      "Training loss: 0.9124531594470253, Trainning Accuracy: 86.66666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.911980280152697, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.911508745850944, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9110385422217883, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9105696550508805, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9101020702281413, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9096357737464191, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9091707517001684, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9087069902841421, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9082444757921048, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9077831946155598, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.907323133242496, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9068642782561485, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9064066163337782, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9059501342454656, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9054948188529218, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9050406571083146, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9045876360531105, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9041357428169321, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9036849646164299, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9032352887541704, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9027867026175381, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9023391936776515, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9018927494882955, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9014473576848655, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9010030059833276, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9005596821791921, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.9001173741464991, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8996760698368208, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8992357572782739, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8987964245745482, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8983580599039448, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8979206515184309, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8974841877427049, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8970486569732741, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8966140476775466, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8961803483929337, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.895747547725965, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8953156343514166, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8948845970114491, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8944544245147587, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.89402510573574, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8935966296136598, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8931689851518425, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8927421614168662, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8923161475377704, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8918909327052754, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8914665061710103, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8910428572467542, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8906199753036875, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8901978497716516, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8897764701384225, Trainning Accuracy: 86.66666666666667\n",
      "Training loss: 0.8893558259489914, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8889359068048587, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8885167023633339, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8880982023368509, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8876803964922882, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8872632746503022, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8868468266846676, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8864310425216315, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8860159121392714, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8856014255668675, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8851875728842824, Trainning Accuracy: 87.5\n",
      "Training loss: 0.8847743442213492, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8843617297572713, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.883949719720028, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8835383043857928, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8831274740783572, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8827172191685662, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8823075300737605, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8818983972572291, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8814898112276688, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8810817625386548, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8806742417881163, Trainning Accuracy: 88.33333333333333\n",
      "Training loss: 0.8802672396178249, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8798607467128875, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8794547538012488, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8790492516532046, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8786442310809187, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8782396829379516, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8778355981187966, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8774319675584219, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8770287822318238, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8766260331535856, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8762237113774448, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8758218079958692, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8754203141396394, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8750192209774406, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8746185197154596, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8742182015969926, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8738182579020586, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8734186799470202, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8730194590842133, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8726205867015838, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8722220542223309, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8718238531045583, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8714259748409338, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8710284109583547, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8706311530176215, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.870234192613118, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8698375213725001, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8694411309563908, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8690450130580812, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8686491594032422, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8682535617496389, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8678582118868557, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.867463101636027, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.867068222849574, Trainning Accuracy: 89.16666666666667\n",
      "Training loss: 0.8666735674109508, Trainning Accuracy: 90.0\n",
      "Training loss: 0.8662791272343952, Trainning Accuracy: 90.0\n",
      "Training loss: 0.8658848942646882, Trainning Accuracy: 90.0\n",
      "Training loss: 0.8654908604769197, Trainning Accuracy: 90.0\n",
      "Training loss: 0.8650970178762606, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8647033584977434, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8643098744060471, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8639165576952926, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8635234004888417, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8631303949391034, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8627375332273495, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8623448075635337, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.86195221018612, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8615597333619165, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8611673693859158, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8607751105811435, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8603829492985118, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8599908779166813, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8595988888419273, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8592069745080163, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8588151273760846, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8584233399345285, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8580316046988957, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8576399142117891, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8572482610427722, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8568566377882844, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8564650370715604, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8560734515425591, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.855681873877896, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.855290296780784, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8548987129809802, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8545071152347392, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8541154963247729, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8537238490602168, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8533321662766032, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8529404408358398, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.852548665626196, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8521568335622943, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8517649375851102, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8513729706619755, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8509809257865912, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8505887959790444, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8501965742858322, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8498042537798938, Trainning Accuracy: 90.83333333333333\n",
      "Training loss: 0.8494118275606448, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8490192887540234, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8486266305125366, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8482338460153193, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8478409284681934, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8474478711037384, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8470546671813646, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8466613099873949, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8462677928351503, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8458741090650455, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8454802520446859, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8450862151689744, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8446919918602231, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8442975755682708, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8439029597706064, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8435081379724999, Trainning Accuracy: 92.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.8431131037071379, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8427178505357646, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8423223720478317, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8419266618611505, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8415307136220533, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8411345210055576, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8407380777155395, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8403413774849093, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8399444140757965, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8395471812797373, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8391496729178707, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8387518828411386, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8383538049304915, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8379554330971009, Trainning Accuracy: 92.5\n",
      "Training loss: 0.837556761282576, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8371577834591875, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8367584936300955, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8363588858295834, Trainning Accuracy: 92.5\n",
      "Training loss: 0.835958954123297, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8355586926084899, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8351580954142725, Trainning Accuracy: 92.5\n",
      "Training loss: 0.834757156701868, Trainning Accuracy: 92.5\n",
      "Training loss: 0.834355870664873, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8339542315295221, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8335522335549596, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8331498710335148, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8327471382909832, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8323440296869116, Trainning Accuracy: 92.5\n",
      "Training loss: 0.831940539614889, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8315366625028425, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8311323928133366, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8307277250438783, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8303226537272261, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8299171734317053, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.829511278761525, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8291049643571012, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.828698224895384, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8282910550901897, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8278834496925346, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8274754034909763, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8270669113119561, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8266579680201471, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8262485685188049, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8258387077501229, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8254283806955913, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8250175823763581, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8246063078535962, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.824194552228871, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8237823106445125, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8233695782839926, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8229563503723009, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8225426221763271, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8221283890052454, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8217136462109005, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8212983891881973, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8208826133754932, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.820466314254991, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8200494873531367, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8196321282410172, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8192142325347616, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8187957958959423, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8183768140319804, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8179572826965511, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8175371976899903, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8171165548597036, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8166953501005769, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8162735793553852, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8158512386152064, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8154283239198324, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8150048313581837, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8145807570687208, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8141560972398609, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8137308481103902, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8133050059698786, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8128785671590946, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8124515280704175, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8120238851482532, Trainning Accuracy: 91.66666666666666\n",
      "Training loss: 0.8115956348894443, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8111667738436849, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8107372986139296, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8103072058568053, Trainning Accuracy: 92.5\n",
      "Training loss: 0.809876492283019, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8094451546577659, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8090131898011352, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8085805945885143, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8081473659509908, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8077135008757532, Trainning Accuracy: 92.5\n",
      "Training loss: 0.807278996406489, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8068438496437791, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8064080577454925, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8059716179271745, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8055345274624356, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8050967836833346, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8046583839807598, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8042193258048057, Trainning Accuracy: 92.5\n",
      "Training loss: 0.803779606665147, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8033392241314086, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8028981758335305, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8024564594621301, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8020140727688582, Trainning Accuracy: 92.5\n",
      "Training loss: 0.801571013566753, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8011272797305864, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8006828691972074, Trainning Accuracy: 92.5\n",
      "Training loss: 0.8002377799658797, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7997920100986137, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7993455577204935, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7988984210199984, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7984505982493173, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7980020877246586, Trainning Accuracy: 92.5\n",
      "Training loss: 0.797552887826553, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7971029970001493, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7966524137555062, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7962011366678731, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7957491643779679, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7952964955922464, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7948431290831625, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7943890636894239, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7939342983162379, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7934788319355512, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7930226635862795, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7925657923745314, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7921082174738211, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7916499381252765, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7911909536378341, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7907312633884281, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7902708668221706, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7898097634525205, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7893479528614447, Trainning Accuracy: 92.5\n",
      "Training loss: 0.78888543469957, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7884222086863241, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7879582746100681, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7874936323282189, Trainning Accuracy: 92.5\n",
      "Training loss: 0.78702828176736, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7865622229233452, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7860954558613884, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7856279807161457, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7851597976917853, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7846909070620481, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7842213091702959, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7837510044295489, Trainning Accuracy: 92.5\n",
      "Training loss: 0.783279993322514, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7828082764015991, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7823358542889184, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7818627276762848, Trainning Accuracy: 92.5\n",
      "Training loss: 0.781388897325192, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7809143640667832, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7804391288018103, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7799631925005801, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7794865562028884, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7790092210179428, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7785311881242727, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7780524587696285, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7775730342708672, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7770929160138269, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7766121054531879, Trainning Accuracy: 92.5\n",
      "Training loss: 0.776130604112322, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7756484135831305, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7751655355258672, Trainning Accuracy: 92.5\n",
      "Training loss: 0.774681971668952, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7741977238087692, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7737127938094548, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7732271836026717, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7727408951873708, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7722539306295405, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7717662920619434, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7712779816838403, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7707890017607021, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7702993546239085, Trainning Accuracy: 92.5\n",
      "Training loss: 0.769809042670434, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7693180683625225, Trainning Accuracy: 92.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.7688264342273478, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7683341428566626, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7678411969064356, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7673475990964744, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7668533522100373, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7663584590934329, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7658629226556071, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7653667458677168, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7648699317626942, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7643724834347962, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7638744040391434, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7633756967912468, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7628763649665224, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7623764118997949, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7618758409847896, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7613746556736108, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7608728594762129, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7603704559598554, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7598674487485515, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7593638415225014, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7588596380175175, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7583548420244386, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7578494573885312, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7573434880088835, Trainning Accuracy: 92.5\n",
      "Training loss: 0.756836937837787, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7563298108801083, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7558221111926507, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7553138428835081, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7548050101114046, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7542956170850301, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7537856680623624, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7532751673499816, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7527641193023762, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7522525283212396, Trainning Accuracy: 92.5\n",
      "Training loss: 0.751740398854758, Trainning Accuracy: 92.5\n",
      "Training loss: 0.75122773539689, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7507145424866388, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7502008247073146, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7496865866857911, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7491718330917541, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7486565686369411, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7481407980743756, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7476245261975945, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7471077578398666, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7465904978734078, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7460727512085872, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7455545227931291, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7450358176113084, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.74451664068314, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7439969970635645, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.743476891841626, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7429563301396493, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7424353171124078, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7419138579462903, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7413919578584625, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7408696220960247, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7403468559351658, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7398236646803146, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7393000536632873, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7387760282424316, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7382515938017697, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7377267557501379, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7372015195203241, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7366758905682034, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7361498743718732, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7356234764307847, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7350967022648756, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7345695574137003, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7340420474355609, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.733514177906636, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7329859544201104, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7324573825853061, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7319284680268119, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7313992163836132, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7308696333082251, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.730339724465824, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.729809495533383, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7292789521988062, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7287481001600667, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7282169451243468, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7276854928071783, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7271537489315887, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7266217192272466, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7260894094296129, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7255568252790937, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7250239725201966, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7244908569006915, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7239574841707745, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7234238600822364, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7228899903876347, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7223558808394713, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7218215371893738, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7212869651872819, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.720752170580639, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7202171591135905, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7196819365261841, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7191465085535796, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7186108809252625, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7180750593642634, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7175390495863849, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7170028572994336, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7164664882024601, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.715929947985004, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.7153932423263474, Trainning Accuracy: 92.5\n",
      "Training loss: 0.714856376894774, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7143193573468369, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7137821893266325, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7132448784650821, Trainning Accuracy: 92.5\n",
      "Training loss: 0.712707430379222, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7121698506715004, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7116321449290827, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7110943187231645, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7105563776082935, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7100183271216987, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7094801727826284, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7089419200916975, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7084035745302415, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7078651415596822, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7073266266208987, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7067880351336101, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7062493724957664, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7057106440829474, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7051718552477728, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7046330113193195, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7040941176025503, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7035551793777501, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7030162018999727, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7024771903984972, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7019381500762932, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7013990861094964, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7008600036468934, Trainning Accuracy: 92.5\n",
      "Training loss: 0.7003209078094172, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6997818036896509, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6992426963513426, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6987035908289291, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6981644921270714, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6976254052201971, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6970863350520555, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6965472865352813, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6960082645509686, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6954692739482549, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6949303195439143, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6943914061219627, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6938525384332704, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.693313721195187, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6927749590911745, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6922362567704514, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6916976188476468, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6911590499024636, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.690620554479352, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6900821370871941, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6895438021989956, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6890055542515905, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6884673976453531, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6879293367439211, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6873913758739282, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6868535193247457, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6863157713482351, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6857781361585082, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6852406179316999, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6847032208057462, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6841659488801758, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6836288062159079, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6830917968350623, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6825549247207748, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6820181938170264, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6814816080284776, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6809451712203135, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6804088872180986, Trainning Accuracy: 93.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6798727598076391, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6793367927348545, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.678800989705658, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6782653543858456, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6777298904009934, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6771946013363646, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6766594907368226, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.676124562106755, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6755898189100039, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6750552645698056, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6745209024687377, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6739867359486742, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6734527683107482, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6729190028153239, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6723854426819738, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6718520910894654, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6713189511757552, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6707860260379893, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6702533187325111, Trainning Accuracy: 92.5\n",
      "Training loss: 0.669720832274878, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6691885696398828, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6686565337615842, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6681247275333424, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6675931538078631, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6670618153972467, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6665307150730456, Trainning Accuracy: 92.5\n",
      "Training loss: 0.665999855566326, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6654692395677384, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6649388697275929, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6644087486559405, Trainning Accuracy: 92.5\n",
      "Training loss: 0.663878878922661, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6633492630575577, Trainning Accuracy: 92.5\n",
      "Training loss: 0.662819903550456, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6622908028513091, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6617619633703095, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6612333874780045, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6607050775054196, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6601770357441847, Trainning Accuracy: 92.5\n",
      "Training loss: 0.659649264446667, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6591217658261089, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6585945420567701, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6580675952740762, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6575409275747698, Trainning Accuracy: 92.5\n",
      "Training loss: 0.657014541017069, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6564884376208275, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6559626193677024, Trainning Accuracy: 92.5\n",
      "Training loss: 0.655437088201324, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6549118460274703, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6543868947142473, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6538622360922711, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6533378719548554, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6528138040582029, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6522900341215995, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6517665638276142, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6512433948222996, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6507205287153989, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6501979670805541, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6496757114555195, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6491537633423763, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6486321242077521, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6481107954830417, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6475897785646338, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6470690748141368, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6465486855586097, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6460286120907955, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6455088556693569, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6449894175191138, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6444702988312847, Trainning Accuracy: 92.5\n",
      "Training loss: 0.643951500763729, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6434330244411925, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6429148709555546, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6423970413660778, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6418795366996588, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6413623579510821, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6408455060832754, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6403289820275657, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6398127866839389, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6392969209212995, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6387813855777319, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6382661814607649, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6377513093476348, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6372367699855525, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6367225640919699, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6362086923548488, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6356951554329294, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6351819539560023, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6346690885251784, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6341565597131616, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6336443680645231, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6331325140959738, Trainning Accuracy: 92.5\n",
      "Training loss: 0.63262099829664, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6321098211283382, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6315989830258513, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6310884843972046, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6305783256239432, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6300685070614096, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6295590290390203, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6290498918605444, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6285410958043823, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6280326411238434, Trainning Accuracy: 92.5\n",
      "Training loss: 0.627524528047425, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6270167567790906, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6265093274985498, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6260022403615348, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6254954955000818, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6249890930228069, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6244830330151867, Trainning Accuracy: 92.5\n",
      "Training loss: 0.6239773155398346, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6234719406367802, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6229669083237467, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6224622185964274, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6219578714287634, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6214538667732207, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6209502045610649, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6204468847026381, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6199439070876338, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6194412715853707, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6189389780450678, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.618437026296117, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6179354161483555, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.617434147392339, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6169332197996121, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6164326331229782, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6159323870967706, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6154324814371204, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6149329158422245, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6144336899926133, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6139348035514165, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6134362561646287, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.612938047461373, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6124401770541652, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.611942644539176, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6114454494964915, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6109485914903741, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6104520700695217, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6099558847673255, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6094600351021268, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.608964520577473, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6084693406823718, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6079744948915453, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6074799826656816, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.606985803451686, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6064919566829303, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6059984417795024, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6055052581484525, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6050124051840385, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6045198822679729, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6040276887696637, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6035358240464572, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6030442874438791, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6025530782958731, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6020621959250396, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6015716396428705, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6010814087499873, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6005915025363714, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.6001019202815988, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5996126612550701, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.59912372471624, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5986351099148455, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5981468160911323, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5976588424760798, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5971711882916255, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5966838527508856, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5961968350583775, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5957101344102377, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.59522374999444, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5947376809910122, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5942519265722498, Trainning Accuracy: 93.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5937664859029295, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5932813581405222, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5927965424354013, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5923120379310522, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5918278437642803, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5913439590654148, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5908603829585148, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5903771145615705, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.589894152986704, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.589411497340371, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.588929146723556, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5884471002319707, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5879653569562481, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5874839159821358, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5870027763906888, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5865219372584584, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5860413976576819, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5855611566564699, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5850812133189914, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5846015667056589, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5841222158733096, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5836431598753877, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5831643977621241, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5826859285807137, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5822077513754917, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5817298651881101, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5812522690577091, Trainning Accuracy: 93.33333333333333\n",
      "Training loss: 0.5807749620210914, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5802979431128898, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5798212113657395, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5793447658104421, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5788686054761345, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5783927293904507, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5779171365796867, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5774418260689608, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5769667968823737, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5764920480431677, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5760175785738819, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5755433874965095, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5750694738326504, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5745958366036646, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5741224748308227, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5736493875354555, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.573176573739102, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5727040324636564, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5722317627315128, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5717597635657097, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5712880339900709, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5708165730293477, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5703453797093578, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.569874453057123, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5694037921010062, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.568933395870846, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5684632633980908, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5679933937159317, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5675237858594315, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5670544388656568, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5665853517738035, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.566116523625326, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5656479534640606, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5651796403363503, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5647115832911671, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5642437813802339, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5637762336581437, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.563308939182478, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5628418970139251, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5623751062163942, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5619085658571311, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5614422750068316, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5609762327397521, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5605104381338212, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5600448902707481, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5595795882361313, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5591145311195647, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5586497180147425, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5581851480195646, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5577208202362376, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5572567337713769, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5567928877361074, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5563292812461619, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5558659134219786, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5554027833887973, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5549398902767555, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5544772332209812, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5540148113616861, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5535526238442564, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5530906698193438, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5526289484429538, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5521674588765341, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5517062002870596, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.55124517184712, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5507843727350015, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5503238021347716, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5498634592363597, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5494033432356382, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5489434533345015, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5484837887409438, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5480243486691375, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5475651323395077, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.547106138978807, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5466473678201901, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5461888181032845, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5457304890742636, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.545272379985915, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.544814490097711, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5443568186758756, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5438993649934512, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5434421283303645, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5429851079734914, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5425283032167189, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5420717133610093, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.54161533771446, Trainning Accuracy: 94.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5411591755923647, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5407032263172707, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5402474892190389, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5397919636348991, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5393366489095067, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5388815443949975, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5384266494510401, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5379719634448896, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5375174857514395, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5370632157532708, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5366091528407028, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.536155296411841, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5357016458726241, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5352482006368714, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5347949601263267, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5343419237707039, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5338890910077296, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.533436461283186, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5329840340509512, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5325318087730405, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5320797849196449, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5316279619691703, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5311763394082738, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5307249167319004, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5302736934433188, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5298226690541552, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5293718430844271, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5289212150625751, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5284707845254953, Trainning Accuracy: 95.0\n",
      "Training loss: 0.528020551018569, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5275705140956927, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5271206733193065, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5266710282604219, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5262215784986485, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5257723236222198, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5253232632280178, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5248743969215974, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5244257243172092, Trainning Accuracy: 95.0\n",
      "Training loss: 0.5239772450378211, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5235289587151402, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5230808649896328, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5226329635105436, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.522185253935914, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5217377359326003, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5212904091762899, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5208432733515165, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5203963281516759, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.51994957327904, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5195030084447689, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5190566333689242, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5186104477804794, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5181644514173314, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5177186440263095, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5172730253631842, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5168275951926748, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5163823532884578, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5159372994331711, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5154924334184211, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5150477550447863, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5146032641218211, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5141589604680591, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5137148439110144, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5132709142871836, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5128271714420457, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5123836152300617, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5119402455146731, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5114970621683008, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5110540650723412, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5106112541171631, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5101686292021029, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5097261902354598, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5092839371344895, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5088418698253974, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5083999882433304, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5079582923323706, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.507516782045523, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5070754573447085, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5066343182007514, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.506193364593369, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5057525965111588, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5053120139515854, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5048716169209676, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5044314054344635, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5039913795160554, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5035515391985336, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5031118845234802, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5026724155412516, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5022331323109605, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5017940349004567, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5013551233863082, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5009163978537805, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.500477858396816, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.5000395051180122, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.4996013381285992, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.49916335754841723, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.49872556350589253, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.49828795613801297, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.4978505355903035, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.49741330201679945, Trainning Accuracy: 94.16666666666667\n",
      "Training loss: 0.4969762555800212, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4965393964509465, Trainning Accuracy: 95.0\n",
      "Training loss: 0.496102724808983, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4956662408419396, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4952299447459978, Trainning Accuracy: 95.0\n",
      "Training loss: 0.49479383672568167, Trainning Accuracy: 95.0\n",
      "Training loss: 0.494357916993828, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4939221857715549, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4934866432882304, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4930512897814403, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4926161254969555, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4921811506886982, Trainning Accuracy: 95.0\n",
      "Training loss: 0.491746365618708, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4913117705571075, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4908773657820666, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4904431515797672, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4900091282443661, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48957529607795847, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48914165539054033, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4887082064999698, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4882749497319287, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4878418854198834, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4874090139050445, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48697633553632663, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48654385067030786, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4861115596711873, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4856794629107438, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48524756076829273, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48481585363064356, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4843843418920554, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4839530259541933, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4835219062260838, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48309098312406906, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4826602570717616, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4822297284999982, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48179939784679265, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4813692655572893, Trainning Accuracy: 95.0\n",
      "Training loss: 0.48093933208371514, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4805095978853314, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4800800634283851, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47965072918606044, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47922159563842875, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47879266327239883, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47836393258166615, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47793540406666285, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47750707823450544, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47707895559894387, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47665103668030917, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4762233220054607, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4757958121077337, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47536850752688536, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47494140880904173, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4745145165066429, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4740878311783891, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4736613533891855, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47323508371008705, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47280902271824254, Trainning Accuracy: 95.0\n",
      "Training loss: 0.47238317099683946, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4719575291350465, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4715320977279579, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4711068773765357, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4706818686875528, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4702570722735353, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46983248875270406, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4694081187489169, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4689839628916096, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4685600218157372, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46813629616171465, Trainning Accuracy: 95.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4677127865753576, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4672894937078217, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46686641821554387, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46644356076018084, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46602092200854905, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4655985026325638, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46517630330917786, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4647543247203206, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4643325675528358, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4639110324984202, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46348972025356155, Trainning Accuracy: 95.0\n",
      "Training loss: 0.463068631519476, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46264776700204613, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4622271274117574, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4618067134636365, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46138652587718715, Trainning Accuracy: 95.0\n",
      "Training loss: 0.46096656537632774, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4605468326893271, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4601273285487418, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45970805369135154, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4592890088580959, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4588701947940101, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4584516122481602, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4580332619735799, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4576151447272051, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45719726126980986, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4567796123659414, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4563621987838553, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45594502129545095, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4555280806762059, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4551113777051116, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45469491316460736, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4542786878405155, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4538627025219759, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45344695800138124, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4530314550743103, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4526161945394636, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45220117719859726, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45178640385645746, Trainning Accuracy: 95.0\n",
      "Training loss: 0.45137187532071543, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4509575924019009, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4505435559133371, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4501297666710748, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4497162254938268, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4493029332029024, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44888989062214146, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4484770985778489, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44806455789872923, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44765226941582054, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44724023396242973, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4468284523740661, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44641692548837675, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44600565414508037, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4455946391859027, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4451838814545109, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4447733817964478, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4443631410590681, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44395316009147207, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4435434397444414, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4431339808703738, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4427247843232189, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44231585095841314, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4419071816328154, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4414987772046429, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44109063853340635, Trainning Accuracy: 95.0\n",
      "Training loss: 0.44068276647984644, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4402751619058696, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4398678256744842, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4394607586497366, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43905396169664845, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43864743568115244, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4382411814700294, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4378351999308458, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43742949193188985, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4370240583421103, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4366189000310526, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43621401786879743, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43580941272589874, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4354050854733214, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4350010369823799, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43459726812467697, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4341937797720423, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43379057279647176, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43338764807006624, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4329850064649719, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43258264885331904, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43218057610716326, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43177878909842454, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4313772886988288, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43097607577984787, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43057515121264106, Trainning Accuracy: 95.0\n",
      "Training loss: 0.43017451586799654, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4297741706162728, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4293741163273405, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4289743538705246, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4285748841145474, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42817570792747073, Trainning Accuracy: 95.0\n",
      "Training loss: 0.427776826176639, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42737823972862327, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4269799494491644, Trainning Accuracy: 95.0\n",
      "Training loss: 0.426581956203117, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42618426085439437, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4257868642659124, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4253897672995351, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42499297081601983, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4245964756749627, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42420028273474486, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4238043928524786, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42340880688395444, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4230135256835875, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42261855010436555, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4222238809977963, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4218295192138556, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42143546560093614, Trainning Accuracy: 95.0\n",
      "Training loss: 0.42104172100579595, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4206482862735079, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4202551622474091, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4198623497690505, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4194698496781483, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4190776628125334, Trainning Accuracy: 95.0\n",
      "Training loss: 0.41868579000810313, Trainning Accuracy: 95.0\n",
      "Training loss: 0.41829423209877287, Trainning Accuracy: 95.0\n",
      "Training loss: 0.41790298991642766, Trainning Accuracy: 95.0\n",
      "Training loss: 0.41751206429087423, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4171214560497944, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4167311660186974, Trainning Accuracy: 95.0\n",
      "Training loss: 0.4163411950208737, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41595154387734895, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4155622134068378, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41517320442569944, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4147845177478915, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41439615418492676, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4140081145458282, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4136203996370859, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41323301026261333, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41284594722370505, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4124592113189936, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41207280334440843, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4116867240931335, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.41130097435556684, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4109155549192791, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4105304665689743, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.410145710086449, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4097612862505535, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40937719583715276, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40899343961908774, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40861001836613753, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4082269328449815, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4078441838191622, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40746177204904854, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40707969829179946, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4066979633013282, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40631656782826664, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40593551261993027, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.405554798420284, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4051744259699077, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4047943960059625, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40441470926215795, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40403536646871874, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40365636835235286, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40327771563621945, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40289940903989724, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.402521449279354, Trainning Accuracy: 95.83333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.40214383706691553, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4017665731112359, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4013896581172676, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4010130927862324, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.4006368778155927, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.40026101389902313, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.3998855017263827, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.39951034198368734, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.39913553535308294, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.398761082512819, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.3983869841372224, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.3980132408966716, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.39763985345757197, Trainning Accuracy: 95.83333333333334\n",
      "Training loss: 0.39726682248233036, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3968941486293315, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39652183255291334, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39614987490334447, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3957782763268005, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3954070374653418, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39503615895689137, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39466564143521315, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39429548552989063, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3939256918663062, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39355626106562114, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39318719374475475, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3928184905163657, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3924501519888327, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3920821787662355, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3917145714483374, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39134733063056687, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39098045690399996, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3906139508553443, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.39024781306692163, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.389882044116652, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3895166445780385, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38915161502015105, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3887869560076126, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38842266810058385, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38805875185474953, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38769520782130495, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38733203654694226, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3869692385738381, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3866068144396412, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38624476467745955, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38588308981585, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38552179037880635, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38516086688574813, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.384800319851511, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3844401497863363, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38408035719586114, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3837209425811096, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3833619064384834, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38300324925975376, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38264497153205296, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38228707373786697, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3819295563550271, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3815724198567044, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.381215664711402, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38085929138294905, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38050330033049484, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.38014769200850335, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.379792466866748, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3794376253503066, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3790831678995571, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3787290949501733, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3783754069331213, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37802210427465577, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37766918739631694, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37731665671492787, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37696451264259184, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3766127555866909, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3762613859498832, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3759104041301022, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37555981052055537, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3752096055097232, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37485978948135934, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3745103628144898, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37416132588341355, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3738126790577026, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37346442270220354, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3731165571770377, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37276908283760324, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3724220000345768, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.372075309113915, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3717290104168575, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3713831042799289, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37103759103494216, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3706924710090013, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37034774452450553, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.37000341189915187, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36965947344594063, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3693159294731786, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3689727802844836, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36863002617879037, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3682876674503544, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3679457043887581, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3676041372789161, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3672629664010812, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36692219203085064, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36658181443917215, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.366241833892351, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3659022506520564, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3655630649753292, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3652242771145888, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36488588731764104, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36454789582768554, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36421030288332457, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3638731087185705, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36353631356285493, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3631999176410374, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36286392117341343, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36252832437572535, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36219312745916993, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36185833063040945, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3615239340915805, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3611899380403044, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.36085634266969757, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3605231481683815, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3601903547204936, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35985796250569824, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35952597169919726, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3591943824717415, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.358863194989642, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3585324094147816, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3582020259046267, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3578720446122393, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3575424656862884, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3572132892710632, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35688451550648503, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35655614452811973, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35622817646719074, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3559006114505918, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3555734496009003, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35524669103639006, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3549203358710451, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35459438421457273, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35426883617241745, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3539436918457751, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35361895133160615, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3532946147226501, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35297068210743976, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35264715357031556, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3523240291914398, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35200130904681165, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35167899320828166, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3513570817435666, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3510355747162645, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.35071447218586976, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3503937742077883, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3500734808333529, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3497535921098387, Trainning Accuracy: 96.66666666666667\n",
      "Training loss: 0.3494341080804788, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34911502878447975, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3487963542570371, Trainning Accuracy: 97.5\n",
      "Training loss: 0.348478084529352, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3481602196286463, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3478427595781793, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34752570439726327, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34720905410128006, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3468928087016974, Trainning Accuracy: 97.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.3465769682060854, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34626153261813264, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3459465019376631, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3456318761606529, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34531765527924674, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3450038392817748, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34469042815276985, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34437742187298376, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3440648204194051, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34375262376527593, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34344083188010877, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34312944472970397, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34281846227616736, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34250788447792724, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34219771128975157, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34188794266276606, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34157857854447093, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3412696188787591, Trainning Accuracy: 97.5\n",
      "Training loss: 0.34096106360593365, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3406529126627251, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3403451659823097, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3400378234943266, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3397308851248962, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33942435079663746, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3391182204286862, Trainning Accuracy: 97.5\n",
      "Training loss: 0.338812493936713, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3385071712329403, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3382022522261617, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33789773682175905, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33759362492172057, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33728991642465944, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33698661122583135, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33668370921715257, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3363812102872187, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33607911432132187, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33577742120147, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33547613080640426, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3351752430116175, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3348747576893722, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3345746747087194, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3342749939355161, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33397571523244407, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3336768384590281, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33337836347165384, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33308029012358664, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3327826182649895, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33248534774294136, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33218847840145554, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3318920100814979, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3315959426210051, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33130027585490307, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3310050096151252, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33071014373063057, Trainning Accuracy: 97.5\n",
      "Training loss: 0.330415678027422, Trainning Accuracy: 97.5\n",
      "Training loss: 0.33012161232856485, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32982794645420493, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3295346802215868, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3292418134450718, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3289493459361565, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3286572775034911, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32836560795289726, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3280743370873862, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3277834647071772, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32749299060971565, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3272029145896911, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3269132364390552, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3266239559470402, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3263350729001765, Trainning Accuracy: 97.5\n",
      "Training loss: 0.326046587082311, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3257584982746251, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32547080625565267, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32518351080129765, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32489661168485273, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3246101086770163, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32432400154591107, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3240382900571017, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3237529739736125, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3234680530559453, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3231835270620968, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32289939574757714, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32261565886542687, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3223323161662346, Trainning Accuracy: 97.5\n",
      "Training loss: 0.322049367398155, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3217668123069261, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3214846506358868, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32120288212599457, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3209215065158425, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32064052354167716, Trainning Accuracy: 97.5\n",
      "Training loss: 0.32035993293741555, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3200797344346632, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3197999277627301, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3195205126486493, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3192414888171933, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3189628559908915, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31868461389004743, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31840676223275527, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3181293007349176, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3178522291102617, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31757554707035707, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31729925432463174, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31702335058038955, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31674783554282665, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31647270891504836, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3161979703980857, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3159236196909123, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3156496564904606, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3153760804916388, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3151028913873468, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3148300888684932, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3145576726240114, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3142856423408755, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3140139977041174, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31374273839684247, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3134718641002454, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3132013744936273, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3129312692544106, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3126615480581558, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31239221057857725, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31212325648755873, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31185468545516937, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3115864971496797, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3113186912375769, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31105126738358113, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31078422525065996, Trainning Accuracy: 97.5\n",
      "Training loss: 0.31051756450004525, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3102512847912474, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30998538578207174, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3097198671286333, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30945472848537187, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30918996950506805, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30892558983885776, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3086615891362476, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3083979670451297, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3081347232117968, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30787185728095745, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3076093688957503, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30734725769775934, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30708552332702865, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3068241654220765, Trainning Accuracy: 97.5\n",
      "Training loss: 0.306563183619911, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30630257755604345, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3060423468645039, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3057824911778546, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3055230101272053, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3052639033422268, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3050051704511656, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30474681108085794, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3044888248567438, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30423121140288134, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30397397034196033, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30371710129531654, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30346060388294516, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30320447772351505, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30294872243438226, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30269333763160344, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30243832292994993, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3021836779429209, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30192940228275694, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3016754955604536, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30142195738577454, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30116878736726477, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30091598511226436, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3006635502269207, Trainning Accuracy: 97.5\n",
      "Training loss: 0.30041148231620257, Trainning Accuracy: 97.5\n",
      "Training loss: 0.3001597809839122, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29990844583269927, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2996574764640727, Trainning Accuracy: 97.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.29940687247841435, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2991566334749915, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2989067590519691, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2986572488064233, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2984081023343531, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2981593192306937, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2979108990893283, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2976628415031006, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29741514606382724, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2971678123623104, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2969208399883491, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29667422853075226, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29642797757735, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29618208671500623, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29593655552963016, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2956913836061884, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2954465705287167, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29520211588033163, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29495801924324233, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2947142801987621, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29447089832731993, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2942278732084721, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29398520442091336, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2937428915424886, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29350093415020384, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2932593318202378, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2930180841279526, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2927771906479055, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2925366509538593, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2922964646187939, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2920566312149168, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29181715031367417, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2915780214857614, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2913392443011344, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2911008183290195, Trainning Accuracy: 97.5\n",
      "Training loss: 0.290862743137925, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2906250182956505, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2903876433692987, Trainning Accuracy: 97.5\n",
      "Training loss: 0.29015061792528485, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2899139415293477, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2896776137465596, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2894416341413365, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28920600227744864, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2889707177180302, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28873578002558986, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2885011887620204, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28826694348860865, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28803304376604577, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28779948915443654, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28756627921330974, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2873334135016272, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2871008915777941, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2868687129996679, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28663687732456844, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28640538410928723, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28617423291009664, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2859434232827597, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2857129547825388, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2854828269642057, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2852530393820499, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2850235915898884, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28479448314107464, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2845657135885072, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2843372824846389, Trainning Accuracy: 97.5\n",
      "Training loss: 0.284109189381486, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2838814338306367, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2836540153832601, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28342693359011484, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28320018800155766, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2829737781675526, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28274770363767854, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2825219639611389, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28229655868676906, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2820714873630454, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2818467495380934, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2816223447596961, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28139827257530203, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28117453253203367, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28095112417669554, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28072804705578214, Trainning Accuracy: 97.5\n",
      "Training loss: 0.28050530071548596, Trainning Accuracy: 97.5\n",
      "Training loss: 0.280282884701706, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2800607985600544, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27983904183586616, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27961761407420505, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2793965148198727, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27917574361741593, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2789553000111339, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2787351835450864, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27851539376310125, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2782959302087815, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27807679242551314, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2778579799564724, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27763949234463314, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27742132913277406, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27720348986348614, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27698597407917974, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2767687813220915, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2765519111342919, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2763353630576919, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27611913663405024, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27590323140497997, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27568764691195613, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27547238269632157, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27525743829929467, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27504281326197566, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2748285071253531, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2746145194303113, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27440084971763606, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27418749752802174, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27397446240207785, Trainning Accuracy: 97.5\n",
      "Training loss: 0.2737617438803352, Trainning Accuracy: 97.5\n",
      "Training loss: 0.27354934150325244, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2733372548112226, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2731254833445792, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.27291402664360276, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2727028842485266, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.27249205569954377, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2722815405368126, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.272071338300463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.27186144853060246, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2716518707673221, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2714426045507029, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.271233649420821, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.27102500491775444, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2708166705815882, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2706086459524204, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.270400930570368, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.27019352397557256, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26998642570820575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2697796353084751, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2695731523166297, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26936697627296524, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2691611067178303, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26895554319163084, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26875028523483685, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26854533238798634, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26834068419169166, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2681363401866444, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26793229991362083, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26772856291348684, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2675251287272034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2673219968958315, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26711916696053717, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26691663846259694, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26671441094340237, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26651248394446503, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26631085700742213, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26610952967404033, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26590850148622164, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26570777198600737, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2655073407155836, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26530720721728535, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26510737103360166, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2649078317071803, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.264708588780832, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2645096417975352, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26431099030044103, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26411263383287714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26391457193835266, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26371680416056237, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2635193300433913, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.263322149130919, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2631252609674239, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.26292866509738755, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.262732361065499, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26253634841665896, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26234062669598385, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2621451954488102, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2619500542206987, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2617552025574381, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2615606400050496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26136636610979047, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26117238041815843, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2609786824768953, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2607852718329911, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26059214803368774, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26039931062648325, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26020675915913494, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.26001449317966396, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25982251223635855, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2596308158777779, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2594394036527556, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2592482751104038, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25905742980011653, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2588668672715731, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2586765870747421, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25848658875988456, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2582968718775576, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25810743597861796, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2579182806142252, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2577294053358455, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25754080969525445, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25735249324454107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25716445553611056, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.256976696122688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2567892145573212, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25660201039338426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2564150831845807, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25622843248494653, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2560420578488533, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2558559588310115, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2556701349864733, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.255484585870636, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2552993110392445, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2551143100483951, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2549295824545374, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2547451278144782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.254560945685384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25437703562478364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2541933971905718, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2540100299410112, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25382693343473567, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.253644107230753, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2534615508884474, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2532792639675828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25309724602830463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2529154966311434, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2527340153370168, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2525528017072325, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25237185530349066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25219117568788646, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25201076242291304, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25183061507146337, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25165073319683307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25147111636272307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25129176413324167, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25111267607290716, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2509338517466501, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.250755290719816, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.250576992558167, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25039895682788504, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2502211830955734, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.25004367092825935, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2498664198933964, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2496894295588664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2495126994929817, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24933622926448754, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.249160018442564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24898406659682812, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24880837329733627, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2486329381145859, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2484577606195177, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2482828403835177, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24810817697841941, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24793376997650557, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2477596189505102, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24758572347362065, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24741208311947926, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24723869746218574, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2470655660762985, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24689268853683702, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24672006441928324, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24654769329958381, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2463755747541515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24620370835986732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24603209369408197, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2458607303346179, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24568961785977067, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2455187558483111, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24534814387948645, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2451777815330224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24500766838912463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24483780402848038, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24466818803226023, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24449881998211934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2443296994601993, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2441608260491297, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24399219933202934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.243823818892508, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24365568431466797, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2434877951831052, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24332015108291127, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24315275159967412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24298559631948013, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24281868482891517, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24265201671506595, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24248559156552152, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24231940896837462, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2421534685122229, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24198776978617026, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.241822312379828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24165709588331652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24149211988726602, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24132738398281817, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24116288776162717, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24099863081586076, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2408346127382017, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24067083312184892, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2405072915605184, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24034398764844464, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24018092098038152, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.24001809115160364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23985549775790704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2396931403956108, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2395310186615576, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.239369132153115, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23920748046817644, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2390460632051623, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2388848799630208, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23872393034122885, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23856321393979343, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23840273035925208, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23824247920067412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2380824600656615, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2379226725563496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23776311627540825, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23760379082604258, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2374446958119938, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23728583083754007, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23712719550749747, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23696878942722066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23681061220260366, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23665266344008082, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23649494274662733, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23633744972976028, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2361801839975392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23602314515856684, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23586633282199, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23570974659749985, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2355533860953333, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2353972509262731, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23524134070164857, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23508565503333667, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23493019353376218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23477495581589847, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2346199414932682, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.234465150179944, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23431058149054865, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2341562350402562, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2340021104447922, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23384820732043418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23369452528401258, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23354106395291094, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23338782294506638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23323480187897042, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23308200037366927, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23292941804876421, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23277705452441227, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23262490942132674, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23247298236077715, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23232127296459038, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23216978085515047, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23201850565539944, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2318674469888375, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2317166044795236, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23156597775207555, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23141556643167063, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23126537014404583, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2311153885154982, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23096562117288524, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2308160677436253, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23066672785569756, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23051760113764275, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23036868721856318, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.23021998572812316, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2300714962965491, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2299232185546302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22977515213371788, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22962729666572707, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22947965178313556, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22933221711898474, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22918499230687953, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22903797698098885, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2288911707760456, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2287445733273469, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22859818427075432, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.228452003242694, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22830602988015672, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22816026382069834, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2280147047024396, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22786935216406637, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2277242058448299, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2275792653845468, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22743453042359907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22729000060293447, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2271456755640662, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22700155494907345, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2268576384006011, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2267139255618598, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22657041607662634, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22642710958924336, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22628400574461946, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22614110418822933, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22599840456611373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22585590652487952, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22571360971169938, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22557151377431234, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22542961836102315, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.225287923120703, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22514642770278867, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22500513175728307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22486403493475496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22472313688633908, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22458243726373564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.224441935719211, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22430163190559693, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2241615254762907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2240216160852551, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2238819033870185, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22374238703667426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22360306668988106, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2234639420028626, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22332501263240737, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2231862782358689, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2230477384711651, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22290939299677864, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22277124147175614, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22263328355570888, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22249551890881186, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22235794719180393, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22222056806598778, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2220833811932294, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2219463862359581, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2218095828571664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2216729707204095, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22153654948980553, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2214003188300348, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22126427840634, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2211284278845258, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22099276693095862, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2208572952125664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2207220123968383, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22058691815182463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22045201214613633, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2203172940489448, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.22018276352998184, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.220048420259539, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21991426390846758, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21978029414817823, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21964651065064078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21951291308838355, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2193795011344937, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21924627446261624, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21911323274695418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2189803756622681, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21884770288387556, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21871521408765127, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21858290895002622, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2184507871479877, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2183188483590789, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21818709226139824, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2180555185335996, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21792412685489146, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2177929169050367, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21766188836435219, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2175310409137086, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21740037423452965, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2172698880087922, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21713958191902544, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21700945564831056, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21687950888028074, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2167497412991202, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21662015258956416, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21649074243689834, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21636151052695832, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2162324565461296, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21610358018134665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21597488112009286, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21584635905039976, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21571801366084706, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2155898446405617, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21546185167921775, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21533403446703575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21520639269478242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21507892605377013, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2149516342358563, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21482451693344315, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21469757383947716, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21457080464744852, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21444420905139078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2143177867458802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21419153742603533, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21406546078751673, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2139395565265261, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.213813824339806, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2136882639246395, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21356287497884918, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21343765720079724, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21331261028938447, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21318773394405016, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2130630278647712, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21293849175206175, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2128141253069728, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21268992823109134, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2125659002265403, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21244204099597755, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2123183502425956, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.212194827670121, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21207147298281384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2119482858854671, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2118252660834063, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2117024132824886, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21157972718910276, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21145720751016803, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21133485395313384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21121266622597942, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21109064403721284, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21096878709587077, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21084709511151778, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21072556779424564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21060420485467302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21048300600394468, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2103619709537309, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21024109941622696, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.21012039110415265, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20999984573075148, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20987946300979018, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20975924265555806, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2096391843828665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2095192879070483, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20939955294395704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2092799792099664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20916056642196987, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20904131429737974, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20892222255412668, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20880329091065922, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20868451908594288, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20856590679945963, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20844745377120766, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20832915972170002, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20821102437196468, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2080930474435434, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20797522865849144, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2078575677393767, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20774006440927928, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20762271839179072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2075055294110133, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20738849719155952, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2072716214585515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20715490193762007, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2070383383549044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20692193043705137, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20680567791121462, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20668958050505418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20657363794673553, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2064578499649294, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2063422162888107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20622673664805793, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20611141077285275, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20599623839387907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20588121924232233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20576635304986923, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20565163954870658, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.205537078471521, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20542266955149796, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20530841252232143, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2051943071181729, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20508035307373082, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20496655012416998, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2048528980051607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20473939645286837, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20462604520395258, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20451284399556635, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2043997925653558, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20428689065145905, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20417413799250586, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20406153432761678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20394907939640242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.203836772938963, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20372461469588726, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20361260440825224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20350074181762215, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20338902666604797, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20327745869606653, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20316603765070015, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20305476327345554, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20294363530832335, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20283265349977736, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20272181759277388, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20261112733275097, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20250058246562774, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20239018273780365, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2022799278961579, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20216981768804843, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20205985186131165, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2019500301642615, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20184035234568848, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2017308181548594, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20162142734151653, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20151217965587667, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20140307484863065, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20129411267094263, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20118529287444917, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.2010766152112588, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20096807943395112, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20085968529557607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20075143254965336, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20064332095017162, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20053535025158772, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20042752020882607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20031983057727779, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20021228111280032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.20010487157171622, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1999976017108127, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19989047128734116, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19978348005901586, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19967662778401374, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19956991422097334, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19946333912899436, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19935690226763678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19925060339692002, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19914444227732245, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.19903841866978061, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19893253233568828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.198826783036896, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19872117053571034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1986156945948928, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1985103549776596, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19840515144768067, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1983000837690789, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1981951517064295, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19809035502475925, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19798569348954578, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19788116686671675, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19777677492264917, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19767251742416878, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19756839413854926, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19746440483351133, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1973605492772222, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1972568272382949, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19715323848578722, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19704978278920138, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19694645991848303, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1968432696440206, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19674021173664463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1966372859676269, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19653449210867974, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1964318299319555, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19632929921004535, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19622689971597918, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1961246312232242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19602249350568482, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19592048633770145, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1958186094940499, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1957168627499408, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19561524588101875, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19551375866336157, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19541240087347955, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19531117228831482, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19521007268524043, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19510910184205987, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19500825953700615, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19490754554874107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19480695965635464, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19470650163936404, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19460617127771332, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1945059683517723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19440589264233593, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19430594393062375, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19420612199827889, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19410642662736732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19400685760037759, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19390741470021938, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19380809771022342, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19370890641414035, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1936098405961401, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19351090004081115, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19341208453315997, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19331339385861, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19321482780300103, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19311638615258864, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19301806869404325, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19291987521444937, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19282180550130523, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19272385934252145, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19262603652642085, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19252833684173742, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19243076007761584, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19233330602361037, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1922359744696845, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19213876520621007, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19204167802396643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19194471271413996, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19184786906832307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19175114687851372, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19165454593711448, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19155806603693198, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1914617069711761, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19136546853345926, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19126935051779567, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19117335271860056, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19107747493068955, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19098171694927804, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1908860785699802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1907905595888083, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1906951598021723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19059987900687875, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1905047170001303, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1904096735795248, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1903147485430548, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19021994168910658, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.19012525281645973, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.190030681724286, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18993622821214917, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18984189208000374, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18974767312819457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18965357115745612, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18955958596891148, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1894657173640721, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18937196514483676, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1892783291134908, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18918480907270563, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18909140482553793, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1889981161754288, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18890494292620338, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18881188488206976, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18871894184761834, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1886261136278215, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18853340002803237, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18844080085398443, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1883483159117907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18825594500794313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18816368794931163, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18807154454314373, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.18797951459706366, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18788759791907161, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18779579431754298, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18770410360122813, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18761252557925093, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18752106006110875, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18742970685667126, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18733846577618002, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18724733663024776, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18715631922985743, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18706541338636176, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18697461891148254, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1868839356173097, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18679336331630092, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18670290182128066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18661255094543974, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18652231050233425, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1864321803058853, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.186342160170378, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1862522499104609, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1861624493411453, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18607275827780456, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18598317653617336, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18589370393234686, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18580434028278048, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18571508540428874, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18562593911404474, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18553690122957947, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1854479715687812, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18535914994989464, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18527043619152034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18518183011261408, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18509333153248597, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1850049402708, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1849166561475732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.184828478983175, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18474040859832677, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1846524448141005, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.184564587451919, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1844768363335545, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18438919128112835, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1843016521171102, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18421421866431734, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18412689074591412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1840396681854112, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18395255080666473, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18386553843387599, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18377863089159044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1836918280046972, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18360512959842837, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1835185354983583, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18343204553040285, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18334565952081902, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18325937729620398, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18317319868349433, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18308712350996598, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1830011516032328, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18291528279124644, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18282951690229543, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1827438537650045, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18265829320833424, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18257283506158, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18248747915437147, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18240222531667205, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18231707337877806, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1822320231713182, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18214707452525286, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18206222727187332, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18197748124280136, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1818928362699884, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18180829218571493, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1817238488225899, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18163950601354995, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18155526359185883, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1814711213911068, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1813870792452098, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1813031369884091, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18121929445527035, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18113555148068314, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18105190789986025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1809683635483371, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18088491826197087, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1808015718769403, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18071832422974457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18063517515720298, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1805521244964542, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1804691720849556, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18038631776048264, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1803035613611283, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18022090272530233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18013834169173074, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.18005587809945497, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17997351178783164, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17989124259653133, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1798090703655386, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17972699493515087, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.179645016145978, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1795631338389416, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17948134785527448, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17939965803652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17931806422453136, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17923656626147105, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17915516398981032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1790738572523282, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17899264589211142, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1789115297525533, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17883050867735348, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17874958251051698, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1786687510963539, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17858801427947857, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17850737190480914, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17842682381756675, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17834636986327498, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.17826600988775934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17818574373714663, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17810557125786422, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1780254922966396, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17794550670049952, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17786561431676962, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17778581499307386, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17770610857733365, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17762649491776755, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17754697386289028, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1774675452615126, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1773882089627403, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1773089648159738, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17722981267090754, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17715075237752934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1770717837861198, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1769929067472516, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17691412111178922, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17683542673088803, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17675682345599383, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1766783111388422, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17659988963145798, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17652155878615466, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17644331845553374, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17636516849248413, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17628710875018166, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1762091390820885, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1761312593419524, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17605346938380617, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17597576906196738, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17589815823103727, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17582063674590062, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1757432044617248, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1756658612339596, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17558860691833628, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1755114413708671, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17543436444784494, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17535737600584236, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17528047590171145, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17520366399258291, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17512694013586547, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1750503041892457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17497375601068701, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17489729545842939, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17482092239098856, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1747446366671556, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17466843814599642, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17459232668685093, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17451630214933278, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1744403643933287, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1743645132789977, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17428874866677094, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1742130704173507, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17413747839171037, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17406197245109317, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1739865524570124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17391121827125025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17383596975585736, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17376080677315273, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1736857291857225, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17361073685641984, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17353582964836428, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17346100742494108, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17338627004980084, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1733116173868587, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1732370493002941, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17316256565454993, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17308816631433235, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17301385114460974, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17293962001061267, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1728654727778331, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17279140931202364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17271742947919758, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17264353314562772, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17256972017784628, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17249599044264408, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17242234380707017, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17234878013843122, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17227529930429109, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17220190117247003, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17212858561104452, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17205535248834636, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17198220167296252, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17190913303373415, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1718361464397566, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17176324176037833, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17169041886520084, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17161767762407792, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17154501790711504, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1714724395846691, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17139994252734758, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17132752660600842, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17125519169175904, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1711829376559562, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17111076437020517, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1710386717063596, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1709666595365204, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17089472773303604, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17082287616850117, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17075110471575677, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17067941324788935, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17060780163823036, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1705362697603558, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17046481748808592, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17039344469548418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17032215125685723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1702509370467542, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17017980193996615, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1701087458115257, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.17003776853670638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16996686999102217, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16989605005022715, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.16982530859031475, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16975464548751742, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1696840606183061, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16961355385938953, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16954312508771424, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16947277418046336, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16940250101505677, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16933230546915018, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1692621874206348, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16919214674763697, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1691221833285173, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16905229704187058, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.168982487766525, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16891275538154188, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1688430997662149, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16877352080007005, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16870401836286458, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16863459233458705, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16856524259545652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16849596902592207, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1684267715066625, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1683576499185858, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16828860414282842, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16821963406075516, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16815073955395843, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16808192050425794, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16801317679369993, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1679445083045572, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16787591491932805, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1678073965207364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1677389529917307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16767058421548384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1676022900753928, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16753407045507773, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16746592523838194, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.167397854309371, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1673298575523327, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1672619348517762, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16719408609243186, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1671263111592507, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16705860993740368, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16699098231228163, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16692342816949454, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16685594739487114, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16678853987445857, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16672120549452163, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16665394414154264, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1665867557022208, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16651964006347175, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16645259711242713, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1663856267364343, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16631872882305548, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16625190326006764, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16618514993546196, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16611846873744346, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16605185955443016, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16598532227505322, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16591885678815607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16585246298279405, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1657861407482341, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1657198899739541, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16565371054964256, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16558760236519826, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16552156531072956, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1654555992765542, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16538970415319867, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16532387983139793, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16525812620209476, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1651924431564397, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16512683058579009, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16506128838171014, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16499581643597008, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.164930414640546, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16486508288761936, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16479982106957644, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1647346290790081, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16466950680870907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16460445415167776, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16453947100111588, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16447455725042767, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16440971279321978, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1643449375233009, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16428023133468095, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.164215594121571, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1641510257783827, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16408652619972794, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16402209528041836, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16395773291546487, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16389343900007744, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16382921342966453, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16376505609983247, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16370096690638553, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16363694574532506, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1635729925128493, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16350910710535294, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16344528941942654, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16338153935185643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.163317856799624, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1632542416599054, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16319069383007112, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16312721320768564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16306379969050702, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1630004531764863, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16293717356376727, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16287396075068608, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16281081463577077, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16274773511774085, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16268472209550688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1626217754681702, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16255889513502228, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1624960809955447, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16243333294940826, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.162370650896473, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1623080347367876, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16224548437058897, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.162182999698302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16212058062053897, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1620582270380993, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16199593885196903, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1619337159633206, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16187155827351224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16180946568408788, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1617474380967763, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1616854754134912, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16162357753633055, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16156174436757634, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16149997580969402, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16143827176533218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16137663213732234, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1613150568286783, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1612535457425959, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1611920987824527, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16113071585180727, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1610693968543993, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1610081416941488, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16094695027515593, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1608858225017005, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1608247582782418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16076375750941796, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16070282010004566, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16064194595511988, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16058113497981338, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16052038707947633, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16045970215963604, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16039908012599646, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.160338520884438, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16027802434101682, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16021759040196482, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.160157218973689, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.16009690996277132, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1600366632759681, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1599764788202098, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15991635650260072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15985629623041836, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1597962979111133, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1597363614523088, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15967648676180032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15961667374755525, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15955692231771254, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15949723238058233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15943760384464553, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15937803661855363, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15931853061112805, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1592590857313601, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15919970188841034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15914037899160854, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15908111695045302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15902191567461035, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15896277507391515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1589036950583697, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15884467553814344, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1587857164235727, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15872681762516033, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15866797905357558, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1586092006196532, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15855048223439375, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15849182380896273, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15843322525469059, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1583746864830721, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15831620740576616, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1582577879345955, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15819942798154613, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15814112745876724, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15808288627857073, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1580247043534308, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15796658159598373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15790851791902752, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15785051323552146, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15779256745858597, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15773468050150197, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15767685227771083, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.157619082700814, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15756137168457235, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15750371914290628, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15744612498989508, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15738858913977669, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1573311115069475, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1572736920059617, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15721633055153125, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15715902705852539, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15710178144197026, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15704459361704892, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15698746349910045, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1569303910036201, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15687337604625876, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15681641854282272, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15675951840927313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15670267556172598, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15664588991645156, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15658916138987422, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15653248989857194, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15647587535927626, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15641931768887163, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15636281680439534, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15630637262303704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15624998506213855, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15619365403919339, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1561373794718467, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15608116127789456, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.156024999375284, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15596889368211259, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15591284411662806, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1558568505972279, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1558009130424594, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.15574503137101897, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15568920550175183, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15563343535365212, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15557772084586205, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1555220618976719, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1554664584285197, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15541091035799073, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15535541760581745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15529998009187906, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15524459773620122, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15518927045895564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15513399818046, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15507878082117751, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15502361830171643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15496851054283015, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15491345746541657, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15485845899051787, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15480351503932036, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.154748625533154, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15469379039349213, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15463900954195123, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15458428290029053, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15452961039041196, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1544749919343594, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15442042745431872, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1543659168726176, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1543114601117247, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15425705709425006, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15420270774294403, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15414841198069787, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1540941697305426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1540399809156492, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1539858454593283, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15393176328502964, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.153877734316342, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15382375847699278, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1537698356908479, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15371596588191128, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15366214897432454, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15360838489236697, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1535546735604551, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1535010149031423, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1534474088451187, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15339385531121072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15334035422638087, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15328690551572755, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15323350910448455, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15318016491802097, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1531268728818409, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.153073632921583, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15302044496302034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15296730893206015, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15291422475474353, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15286119235724496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15280821166587233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1527552826070665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.152702405107401, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15264957909358176, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15259680449244706, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15254408123096685, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15249140923624271, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15243878843550776, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15238621875612596, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15233370012559208, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15228123247153155, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15222881572169983, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15217644980398257, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15212413464639496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1520718701770817, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15201965632431663, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15196749301650242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15191538018217052, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15186331774998052, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15181130564872025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15175934380730538, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15170743215477894, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15165557062031146, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15160375913320043, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15155199762287003, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15150028601887094, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1514486242508802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15139701224870078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15134544994226118, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1512939372616155, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1512424741369431, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15119106049854808, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15113969627685936, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15108838140243014, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15103711580593784, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1509858994181838, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15093473217009287, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15088361399271338, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1508325448172168, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15078152457489732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.150730553197172, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1506796306155799, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1506287567617825, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15057793156756294, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15052715496482594, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15047642688559779, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15042574726202557, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15037511602637738, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15032453311104194, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1502739984485281, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.150223511971465, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1501730736126016, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.15012268330480633, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1500723409810671, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1500220465744908, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1499718000183032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14992160124584886, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14987145019059037, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.14982134678610862, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14977129096610248, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14972128266438817, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14967132181489945, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1496214083516873, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14957154220891944, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14952172332088032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1494719516219708, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14942222704670788, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1493725495297245, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14932291900576936, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14927333540970653, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1492237986765153, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1491743087412901, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14912486553923976, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14907546900568797, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14902611907607255, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14897681568594526, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14892755877097177, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14887834826693122, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1488291841097163, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1487800662353324, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14873099457989816, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14868196907964465, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14863298967091526, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14858405629016586, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14853516887396387, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1484863273589886, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14843753168203092, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14838878177999268, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1483400775898871, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14829141904883786, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1482428060940793, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1481942386629562, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14814571669292328, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14809724012154526, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1480488088864964, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14800042292556045, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14795208217663045, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1479037865777082, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14785553606690438, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1478073305824382, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14775917006263714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14771105444593682, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1476629836708806, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1476149576761196, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14756697640041233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14751903978262437, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1474711477617285, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14742330027680406, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14737549726703705, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14732773867171972, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14728002443025048, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1472323544821335, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14718472876697883, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14713714722450172, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14708960979452282, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14704211641696782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14699466703186706, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14694726157935561, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14689989999967287, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14685258223316233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14680530822027157, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14675807790155176, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14671089121765768, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14666374810934732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.146616648517482, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1465695923830257, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14652257964704513, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14647561025070951, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14642868413529025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14638180124216096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14633496151279687, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14628816488877505, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14624141131177393, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14619470072357302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14614803306605306, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14610140828119544, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14605482631108224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1460082870978959, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14596179058391906, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14591533671153425, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14586892542322394, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14582255666157012, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14577623036925408, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1457299464890564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1456837049638566, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14563750573663287, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14559134875046206, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14554523394851943, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14549916127407828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14545313067050997, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14540714208128364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14536119544996592, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14531529072022084, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14526942783580968, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1452236067405905, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14517782737851848, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14513208969364505, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1450863936301181, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1450407391321819, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14499512614417648, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1449495546105377, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14490402447579723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14485853568458187, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14481308818161392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14476768191171044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14472231681978348, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1446769928508397, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14463170994998018, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14458646806240025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14454126713338936, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1444961071083308, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14445098793270147, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.144405909552072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14436087191210603, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14431587495856044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1442709186372851, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1442260028942225, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1441811276754078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1441362929269685, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1440914985951242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14404674462618652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14400203096655895, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14395735756273648, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14391272436130564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14386813130894427, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14382357835242116, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1437790654385958, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1437345925144188, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.143690159526931, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14364576642326365, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1436014131506381, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1435570996563658, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14351282588784778, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14346859179257485, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14342439731812717, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14338024241217415, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14333612702247436, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14329205109687507, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14324801458331238, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14320401742981093, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14316005958448363, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14311614099553166, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1430722616112441, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14302842137999788, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14298462025025765, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14294085817057536, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14289713508959043, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14285345095602922, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1428098057187052, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14276619932651843, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14272263172845576, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14267910287359017, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14263561271108124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14259216119017437, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14254874826020086, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14250537387057788, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14246203797080798, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14241874051047923, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14237548143926482, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14233226070692304, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14228907826329704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14224593405831457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14220282804198803, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14215976016441412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1421167303757738, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.14207373862633188, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14203078486643722, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14198786904652222, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14194499111710296, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14190215102877868, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14185934873223185, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14181658417822818, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14177385731761588, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14173116810132608, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1416885164803724, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14164590240585068, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14160332582893914, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14156078670089797, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14151828497306912, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14147582059687636, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14143339352382497, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1413910037055016, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14134865109357397, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1413063356397912, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1412640572959829, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14122181601405967, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1411796117460126, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14113744444391313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14109531405991305, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14105322054624417, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14101116385521822, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1409691439392268, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14092716075074094, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1408852142423113, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14084330436656767, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1408014310762192, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14075959432405372, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14071779406293808, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14067603024581785, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1406343028257169, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14059261175573762, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14055095698906053, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1405093384789442, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1404677561787251, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14042621004181735, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1403847000217128, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14034322607198066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14030178814626731, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1402603861982964, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1402190201818684, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14017769005086075, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14013639575922743, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14009513726099904, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1400539145102823, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.14001272746126042, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1399715760681925, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13993046028541362, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13988938006733456, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13984833536844174, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13980732614329697, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1397663523465375, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13972541393287555, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1396845108570986, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13964364307406868, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13960281053872284, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13956201320607242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13952125103120339, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13948052396927585, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13943983197552418, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13939917500525656, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13935855301385508, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1393179659567756, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13927741378954733, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1392368964677731, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13919641394712876, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13915596618336354, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13911555313229934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1390751747498311, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13903483099192643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13899452181462527, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13895424717404015, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13891400702635578, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13887380132782898, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13883363003478855, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13879349310363512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13875339049084096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13871332215294985, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13867328804657714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13863328812840928, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13859332235520383, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13855339068378955, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13851349307106572, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13847362947400257, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13843379984964085, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1383940041550916, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13835424234753638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13831451438422668, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1382748202224841, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1382351598197001, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13819553313333596, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13815594012092247, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1381163807400599, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13807685494841787, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1380373627037352, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13799790396381972, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13795847868654826, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1379190868298664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13787972835178838, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13784040321039695, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1378011113638433, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13776185277034678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13772262738819496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1376834351757434, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13764427609141539, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13760515009370203, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13756605714116202, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13752699719242156, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1374879702061741, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.13744897614118026, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13741001495626784, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13737108661033148, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1373321910623327, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1372933282712996, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13725449819632693, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13721570079657575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13717693603127348, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13713820385971373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.137099504241256, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13706083713532594, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13702220250141461, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13698360029907905, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13694503048794165, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1369064930276902, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1368679878780778, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13682951499892268, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1367910743501081, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13675266589158205, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13671428958335757, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.136675945385512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13663763325818742, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13659935316159022, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13656110505599103, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13652288890172468, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13648470465918994, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13644655228884947, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1364084317512297, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13637034300692072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13633228601657613, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1362942607409129, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1362562671407113, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1362183051768147, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1361803748101296, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13614247600162527, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1361046087123338, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13606677290335, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13602896853583124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1359911955709972, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13595345397012987, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1359157436945735, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13587806470573435, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13584041696508067, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13580280043414247, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13576521507451145, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13572766084784096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13569013771584573, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1356526456403019, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13561518458304683, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13557775450597898, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13554035537105774, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1355029871403035, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1354656497757973, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13542834323968092, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1353910674941565, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13535382250148673, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1353166082239946, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13527942462406317, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1352422716641356, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1352051493067151, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13516805751436453, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1351309962497066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13509396547542352, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1350569651542571, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13501999524900848, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13498305572253796, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13494614653776515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13490926765766859, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1348724190452857, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13483560066371283, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13479881247610498, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13476205444567554, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1347253265356967, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1346886287094988, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13465196093047044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13461532316205832, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13457871536776742, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13454213751116031, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13450558955585745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13446907146553708, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13443258320393495, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13439612473484433, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13435969602211575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13432329702965717, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13428692772143347, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13425058806146678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.134214278013836, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13417799754267687, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13414174661218198, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1341055251866003, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13406933323023745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13403317070745538, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13399703758267228, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13396093382036267, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13392485938505694, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13388881424134152, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13385279835385863, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13381681168730641, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13378085420643854, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.133744925876064, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1337090266610477, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13367315652630934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1336373154368242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13360150335762253, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13356572025378963, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13352996609046566, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13349424083284564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13345854444617927, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1334228768957707, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13338723814697884, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13335162816521678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13331604691595192, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.1332804943647059, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13324497047705447, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13320947521862714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13317400855510758, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1331385704522331, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13310316087579457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13306777979163661, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13303242716565722, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13299710296380773, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13296180715209288, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1329265396965704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1328913005633512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13285608971859905, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13282090712853073, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13278575275941565, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13275062657757597, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13271552854938634, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13268045864127392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13264541681971828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1326104030512512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13257541730245664, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13254045953997062, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13250552973048116, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13247062784072824, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13243575383750333, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13240090768764995, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.132366089358063, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13233129881568886, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13229653602752536, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13226180096062165, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13222709358207796, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13219241385904576, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13215776175872745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13212313724837632, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13208854029529649, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1320539708668429, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.132019428930421, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13198491445348673, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13195042740354665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13191596774815745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1318815354549262, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13184713049151012, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13181275282561639, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13177840242500224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13174407925747478, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13170978329089078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13167551449315681, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13164127283222907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13160705827611313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13157287079286403, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13153871035058615, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13150457691743303, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13147047046160734, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1314363909513609, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13140233835499432, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13136831264085724, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13133431377734786, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13130034173291322, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1312663964760489, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13123247797529894, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13119858619925576, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13116472111656016, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1311308826959012, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13109707090601588, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13106328571568948, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13102952709375507, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1309957950090936, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13096208943063392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1309284103273524, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13089475766827313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13086113142246766, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13082753155905497, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1307939580472013, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13076041085612025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1307268899550726, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13069339531336605, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13065992690035538, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13062648468544227, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13059306863807515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1305596787277493, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13052631492400643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.130492977196435, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13045966551466978, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.130426379848392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13039312016732926, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13035988644125512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13032667863998956, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13029349673339835, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1302603406913933, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1302272104839321, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13019410608101825, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1301610274527009, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1301279745690748, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1300949474002802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13006194591650286, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.13002897008797396, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12999601988496973, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1299630952778118, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12993019623686686, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12989732273254656, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12986447473530754, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12983165221565127, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12979885514412412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12976608349131696, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12973333722786543, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12970061632444957, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.129667920751794, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12963525048066762, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12960260548188363, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1295699857262994, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12953739118481658, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12950482182838066, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1294722776279812, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12943975855465156, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.12940726457946897, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12937479567355442, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12934235180807244, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12930993295423115, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12927753908328218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1292451701665205, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12921282617528448, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12918050708095563, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1291482128549587, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12911594346876154, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1290836988938749, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1290514791018525, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12901928406429097, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12898711375282965, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12895496813915053, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1289228471949783, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1288907508920801, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12885867920226562, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12882663209738682, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12879460954933802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12876261153005575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1287306380115188, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12869868896574774, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1286667643648054, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1286348641807965, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12860298838586742, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1285711369522065, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12853930985204365, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1285075070576505, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12847572854134, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1284439742754669, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12841224423242698, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12838053838465757, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1283488567046371, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12831719916488527, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1282855657379628, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12825395639647139, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12822237111305373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1281908098603933, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12815927261121451, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12812775933828233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12809627001440246, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1280648046124211, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12803336310522492, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1280019454657412, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12797055166693735, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12793918168182114, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1279078354834405, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12787651304488354, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12784521433927837, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12781393933979313, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12778268801963583, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12775146035205434, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12772025631033626, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12768907586780892, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12765791899783924, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12762678567383373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12759567586923828, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12756458955753835, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1275335267122586, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12750248730696295, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1274714713152546, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12744047871077582, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12740950946720794, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1273785635582713, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12734764095772502, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12731674163936726, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12728586557703478, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1272550127446032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1272241831159866, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12719337666513775, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12716259336604782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1271318331927465, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12710109611930173, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12707038211981983, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12703969116844524, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1270090232393607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12697837830678688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12694775634498243, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12691715732824407, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12688658123090635, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12685602802734164, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12682549769196, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1267949901992092, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12676450552357463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12673404363957907, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.126703604521783, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12667318814478415, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1266427944832176, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12661242351175575, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1265820752051081, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12655174953802145, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12652144648527947, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12649116602170293, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1264609081221496, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12643067276151398, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1264004599147275, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12637026955675823, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1263401016626109, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12630995620732702, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12627983316598443, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1262497325136976, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1262196542256172, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12618959827693055, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12615956464286102, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12612955329866823, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12609956421964802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12606959738113233, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12603965275848897, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12600973032712184, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1259798300624708, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1259499519400113, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.12592009593525483, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12589026202374837, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12586045018107464, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12583066038285196, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1258008926047341, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12577114682241028, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12574142301160515, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12571172114807871, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12568204120762613, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12565238316607782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12562274699929932, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12559313268319128, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12556354019368934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12553396950676407, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12550442059842093, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12547489344470025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12544538802167704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12541590430546112, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12538644227219692, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1253570018980633, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12532758315927392, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12529818603207662, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12526881049275382, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12523945651762217, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1252101240830327, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12518081316537052, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12515152374105504, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12512225578653963, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1250930092783118, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12506378419289294, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1250345805068384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12500539819673748, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12497623723921303, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12494709761092189, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12491797928855444, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12488888224883474, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12485980646852024, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12483075192440214, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12480171859330495, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12477270645208646, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12474371547763802, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12471474564688399, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12468579693678213, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12465686932432322, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12462796278653117, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12459907730046293, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12457021284320842, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12454136939189049, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12451254692366474, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12448374541571969, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12445496484527657, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12442620518958923, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12439746642594426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12436874853166069, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12434005148409011, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12431137526061665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12428271983865671, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12425408519565906, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12422547130910483, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12419687815650728, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12416830571541189, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12413975396339627, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12411122287807003, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12408271243707486, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12405422261808426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12402575339880384, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12399730475697078, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12396887667035432, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12394046911675514, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12391208207400585, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12388371551997047, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1238553694325447, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12382704378965567, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12379873856926202, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12377045374935373, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12374218930795215, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12371394522310987, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12368572147291079, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12365751803546997, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12362933488893342, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12360117201147844, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12357302938131329, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1235449069766771, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12351680477584, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12348872275710292, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12346066089879756, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12343261917928638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12340459757696266, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12337659607025017, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1233486146376032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1233206532575068, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1232927119084763, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12326479056905756, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12323688921782673, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12320900783339037, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12318114639438528, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1231533048794784, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12312548326736691, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12309768153677811, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12306989966646929, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12304213763522782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.123014395421871, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12298667300524599, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12295897036422974, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12293128747772927, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12290362432468109, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12287598088405144, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12284835713483626, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12282075305606116, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1227931686267811, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.12276560382608064, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12273805863307383, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12271053302690405, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.122683026986744, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12265554049179567, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12262807352129039, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12260062605448852, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12257319807067969, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12254578954918253, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12251840046934473, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.122491030810543, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12246368055218299, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12243634967369914, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1224090381545549, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12238174597424234, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12235447311228234, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12232721954822451, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12229998526164702, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12227277023215667, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12224557443938884, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12221839786300731, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1221912404827044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12216410227820074, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12213698322924543, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12210988331561566, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12208280251711717, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1220557408135836, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1220286981848769, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12200167461088711, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12197467007153229, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12194768454675857, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12192071801653999, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12189377046087849, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12186684185980395, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12183993219337395, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12181304144167394, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12178616958481714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12175931660294426, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12173248247622377, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1217056671848517, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12167887070905165, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12165209302907458, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1216253341251991, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12159859397773096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12157187256700343, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12154516987337706, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12151848587723965, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12149182055900612, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12146517389911866, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12143854587804649, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12141193647628598, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1213853456743605, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12135877345282031, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12133221979224268, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12130568467323181, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12127916807641866, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12125266998246095, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12122619037204323, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12119972922587678, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12117328652469939, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12114686224927564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12112045638039654, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1210940688988797, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12106769978556914, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12104134902133538, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12101501658707531, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12098870246371209, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12096240663219533, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12093612907350071, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12090986976863027, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12088362869861213, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12085740584450053, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12083120118737585, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1208050147083445, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12077884638853875, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12075269620911697, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12072656415126334, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1207004501961879, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12067435432512656, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12064827651934096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12062221676011842, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1205961750287721, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12057015130664057, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12054414557508819, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12051815781550476, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12049218800930563, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12046623613793167, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12044030218284904, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1204143861255494, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12038848794754975, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12036260763039229, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12033674515564453, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12031090050489919, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12028507365977416, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1202592646019125, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12023347331298218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12020769977467645, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12018194396871346, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1201562058768362, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1201304854808128, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12010478276243604, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12007909770352372, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12005343028591825, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12002778049148698, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.12000214830212183, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11997653369973946, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11995093666628107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11992535718371251, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1198997952340242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11987425079923103, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11984872386137228, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11982321440251173, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11979772240473754, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11977224785016218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11974679072092242, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11972135099917933, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11969592866711808, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11967052370694815, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11964513610090306, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1196197658312405, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11959441288024218, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1195690772302138, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11954375886348509, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1195184577624096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11949317390936494, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11946790728675245, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11944265787699736, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1194174256625486, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11939221062587892, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11936701274948466, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11934183201588594, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11931666840762642, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1192915219072734, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11926639249741763, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11924128016067342, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11921618487967847, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11919110663709409, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11916604541560472, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1191410011979183, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11911597396676604, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1190909637049025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11906597039510529, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11904099402017534, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11901603456293669, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11899109200623652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11896616633294506, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11894125752595563, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11891636556818443, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11889149044257072, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11886663213207663, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11884179061968725, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11881696588841048, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11879215792127688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1187673667013401, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11874259221167623, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11871783443538414, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11869309335558548, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11866836895542433, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11864366121806753, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11861897012670436, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11859429566454657, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11856963781482852, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11854499656080694, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11852037188576085, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11849576377299179, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.11847117220582361, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11844659716760238, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11842203864169637, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1183974966114962, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11837297106041457, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11834846197188638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11832396932936862, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11829949311634032, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11827503331630256, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11825058991277837, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11822616288931288, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11820175222947296, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11817735791684753, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11815297993504725, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11812861826770463, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11810427289847401, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11807994381103137, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11805563098907443, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1180313344163227, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1180070540765172, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11798278995342054, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11795854203081703, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11793431029251239, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11791009472233391, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11788589530413018, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11786171202177152, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11783754485914934, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11781339380017652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1177892588287874, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11776513992893729, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11774103708460307, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11771695027978267, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11769287949849522, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11766882472478102, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11764478594270147, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11762076313633908, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11759675628979732, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1175727653872008, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11754879041269499, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11752483135044633, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11750088818464223, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11747696089949088, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11745304947922137, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11742915390808359, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11740527417034814, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11738141025030652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11735756213227069, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11733372980057347, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1173099132395683, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11728611243362913, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1172623273671505, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11723855802454762, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11721480439025597, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11719106644873177, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11716734418445143, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11714363758191192, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11711994662563048, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11709627130014479, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11707261159001275, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11704896747981257, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11702533895414273, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11700172599762179, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11697812859488863, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11695454673060221, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11693098038944157, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11690742955610585, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11688389421531425, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11686037435180592, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11683686995034014, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11681338099569591, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1167899074726723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11676644936608822, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11674300666078248, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11671957934161356, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11669616739345995, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11667277080121966, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11664938954981054, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11662602362417017, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11660267300925564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1165793376900439, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11655601765153124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11653271287873364, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11650942335668665, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11648614907044522, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11646289000508386, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11643964614569643, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11641641747739628, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11639320398531607, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11637000565460782, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1163468224704429, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11632365441801192, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11630050148252472, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11627736364921044, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11625424090331737, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11623113323011292, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11620804061488361, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11618496304293523, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11616190049959238, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11613885297019885, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11611582044011746, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11609280289472992, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11606980031943695, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11604681269965811, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11602384002083187, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11600088226841564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11597793942788552, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11595501148473646, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1159320984244822, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11590920023265519, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11588631689480658, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1158634483965062, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11584059472334249, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11581775586092255, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.115794931794872, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11577212251083513, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1157493279944747, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11572654823147185, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11570378320752639, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11568103290835638, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1156582973196984, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1156355764273073, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11561287021695651, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11559017867443745, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11556750178556013, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11554483953615262, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11552219191206128, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11549955889915074, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11547694048330372, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11545433665042114, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11543174738642194, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11540917267724327, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11538661250884025, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11536406686718609, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11534153573827194, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.115319019108107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11529651696271834, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11527402928815096, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11525155607046784, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11522909729574964, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11520665295009502, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11518422301962028, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11516180749045975, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1151394063487652, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1151170195807063, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1150946471724704, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11507228911026243, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11504994538030501, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.11502761596883841, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11500530086212042, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11498300004642635, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11496071350804912, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11493844123329904, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11491618320850397, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11489393942000915, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11487170985417736, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11484949449738853, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1148272933360402, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11480510635654712, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11478293354534125, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11476077488887199, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11473863037360593, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11471649998602688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11469438371263578, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11467228153995083, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11465019345450737, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11462811944285774, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11460605949157146, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11458401358723512, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11456198171645227, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11453996386584354, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1145179600220465, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1144959701717156, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1144739943015224, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1144520323981552, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11443008444831926, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11440815043873655, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11438623035614606, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11436432418730341, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11434243191898107, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11432055353796822, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11429868903107077, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11427683838511125, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11425500158692899, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11423317862337982, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11421136948133626, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11418957414768735, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1141677926093388, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1141460248532127, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11412427086624775, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11410253063539913, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11408080414763838, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1140590913899536, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11403739234934919, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11401570701284601, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11399403536748116, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11397237740030813, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11395073309839687, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11392910244883321, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11390748543871965, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11388588205517465, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11386429228533299, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11384271611634557, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11382115353537943, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11379960452961779, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11377806908625997, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11375654719252129, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1137350388356331, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11371354400284299, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11369206268141423, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11367059485862631, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11364914052177458, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11362769965817034, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11360627225514071, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11358485830002879, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11356345778019344, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11354207068300942, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11352069699586727, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11349933670617326, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1134779898013495, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11345665626883372, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11343533609607945, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11341402927055583, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11339273577974769, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11337145561115551, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1133501887522953, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11332893519069874, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11330769491391302, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11328646790950088, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11326525416504053, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11324405366812572, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11322286640636564, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11320169236738495, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11318053153882361, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11315938390833714, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11313824946359627, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11311712819228723, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.1130960200821114, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11307492512078551, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11305384329604173, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11303277459562719, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11301171900730446, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11299067651885124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11296964711806036, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11294863079273997, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11292762753071314, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11290663731981816, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11288566014790839, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11286469600285232, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11284374487253332, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11282280674484986, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11280188160771548, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11278096944905865, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11276007025682258, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11273918401896571, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11271831072346124, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11269745035829722, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11267660291147659, Trainning Accuracy: 98.33333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.11265576837101716, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11263494672495146, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11261413796132688, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11259334206820552, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11257255903366432, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11255178884579478, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11253103149270324, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11251028696251068, Trainning Accuracy: 98.33333333333333\n",
      "Training loss: 0.11248955524335265, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11246883632337938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11244813019075577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11242743683366119, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11240675624028972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11238608839884977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11236543329756449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11234479092467135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11232416126842237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11230354431708406, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11228294005893724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11226234848227727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11224176957541383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11222120332667097, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.112200649724387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11218010875691468, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11215958041262102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11213906467988727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.112118561547109, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11209807100269596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11207759303507209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1120571276326756, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11203667478395875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11201623447738808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11199580670144414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11197539144462164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11195498869542941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11193459844239023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11191422067404096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11189385537893261, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11187350254562993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11185316216271184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11183283421877117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1118125187024147, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11179221560226302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1117719249069507, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11175164660512621, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11173138068545173, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11171112713660343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11169088594727115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11167065710615864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11165044060198326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11163023642347629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11161004455938259, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11158986499846078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11156969772948316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11154954274123567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11152940002251795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11150926956214319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11148915134893811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11146904537174329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11144895161941253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11142887008081333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11140880074482673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11138874360034721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11136869863628276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11134866584155478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11132864520509814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11130863671586112, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11128864036280539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11126865613490604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11124868402115136, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11122872401054319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11120877609209652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11118884025483974, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11116891648781442, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11114900478007546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11112910512069096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11110921749874221, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11108934190332381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11106947832354332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11104962674852172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1110297871673929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11100995956930396, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1109901439434151, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11097034027889964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11095054856494384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11093076879074705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1109110009455217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1108912450184931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1108715009988997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11085176887599273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11083204863903648, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11081234027730806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11079264378009768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11077295913670816, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11075328633645536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11073362536866796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1107139762226874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11069433888786806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11067471335357691, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11065509960919384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11063549764411146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11061590744773503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11059632900948264, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11057676231878494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11055720736508536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11053766413783996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11051813262651737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11049861282059889, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11047910470957843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11045960828296243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11044012353026998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11042065044103251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11040118900479425, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.11038173921111166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11036230104955388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11034287450970245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11032345958115133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11030405625350695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11028466451638809, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11026528435942605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1102459157722643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11022655874455888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.110207213265978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11018787932620233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11016855691492472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11014924602185033, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11012994663669663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11011065874919336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11009138234908233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11007211742611768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1100528639700658, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11003362197070515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.11001439141782628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10999517230123212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1099759646107375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10995676833616937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10993758346736685, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10991840999418111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10989924790647532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10988009719412473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10986095784701654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10984182985505005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10982271320813637, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10980360789619874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10978451390917228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1097654312370039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10974635986965267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10972729979708937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10970825100929671, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10968921349626924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10967018724801329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10965117225454712, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10963216850590071, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1096131759921159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10959419470324619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10957522462935693, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10955626576052512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10953731808683953, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10951838159840052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10949945628532042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10948054213772282, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10946163914574326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10944274729952883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10942386658923815, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10940499700504148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10938613853712073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10936729117566929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10934845491089208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10932962973300563, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10931081563223792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10929201259882843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10927322062302812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10925443969509942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10923566980531613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10921691094396367, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1091981631013386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10917942626774911, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10916070043351464, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.109141985588966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10912328172444538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10910458883030627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10908590689691344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10906723591464303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1090485758738824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1090299267650302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10901128857849628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10899266130470175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10897404493407897, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10895543945707142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10893684486413377, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10891826114573186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10889968829234276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10888112629445452, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10886257514256636, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10884403482718875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10882550533884298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10880698666806159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10878847880538806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10876998174137699, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10875149546659395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10873301997161555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1087145552470293, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10869610128343378, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10867765807143848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10865922560166379, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10864080386474102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10862239285131248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10860399255203133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10858560295756148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1085672240585779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10854885584576623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1085304983098231, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10851215144145576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10849381523138239, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1084754896703319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10845717474904401, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1084388704582691, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10842057678876837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10840229373131371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10838402127668774, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1083657594156836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10834750813910539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1083292674377676, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10831103730249551, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10829281772412497, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10827460869350246, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10825641020148502, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10823822223894032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10822004479674656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10820187786579245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10818372143697737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10816557550121102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10814744004941379, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1081293150725164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10811120056146019, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1080930965071968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1080750029006885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10805691973290782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10803884699483773, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1080207846774717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10800273277181346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1079846912688772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10796666015968741, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10794863943527896, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10793062908669697, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10791262910499685, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10789463948124457, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10787666020651597, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10785869127189744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10784073266848553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10782278438738702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10780484641971888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10778691875660844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10776900138919297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10775109430862011, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10773319750604762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10771531097264336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10769743469958536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1076795686780617, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10766171289927069, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10764386735442066, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10762603203472992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10760820693142706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10759039203575042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1075725873389487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10755479283228038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10753700850701402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10751923435442813, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10750147036581126, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10748371653246197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10746597284568853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10744823929680936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10743051587715276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10741280257805685, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10739509939086972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10737740630694932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1073597233176634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10734205041438963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10732438758851554, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1073067348314383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10728909213456515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10727145948931287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10725383688710817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10723622431938745, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10721862177759697, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10720102925319253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10718344673763985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10716587422241428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10714831169900071, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10713075915889402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10711321659359853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10709568399462827, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10707816135350691, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10706064866176776, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10704314591095375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10702565309261729, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10700817019832058, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10699069721963526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10697323414814251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10695578097543311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10693833769310739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10692090429277508, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10690348076605555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10688606710457758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10686866329997946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10685126934390891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10683388522802316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10681651094398881, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10679914648348192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10678179183818791, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10676444699980171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10674711196002748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10672978671057887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1067124712431788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10669516554955968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10667786962146304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1066605834506399, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10664330702885047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10662604034786434, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10660878339946034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10659153617542647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10657429866756021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1065570708676681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10653985276756588, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10652264435907864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10650544563404057, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10648825658429509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10647107720169474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10645390747810135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10643674740538576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10641959697542792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10640245618011714, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10638532501135153, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10636820346103854, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10635109152109454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10633398918344505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10631689644002468, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10629981328277703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10628273970365472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10626567569461945, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10624862124764188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10623157635470168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10621454100778746, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10619751519889695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10618049892003664, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10616349216322213, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1061464949204778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10612950718383708, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1061125289453422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10609556019704446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10607860093100381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10606165113928928, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10604471081397855, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10602777994715833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1060108585309241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10599394655738015, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10597704401863955, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10596015090682424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10594326721406487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10592639293250095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10590952805428058, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10589267257156083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10587582647650737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1058589897612946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10584216241810566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1058253444391324, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10580853581657532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10579173654264362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10577494660955515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1057581660095364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10574139473482251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10572463277765734, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10570788013029314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10569113678499098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10567440273402044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10565767796965964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10564096248419536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1056242562699228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10560755931914587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10559087162417681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10557419317733663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1055575239709546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10554086399736863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10552421324892514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10550757171797893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10549093939689323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10547431627803988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.105457702353799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10544109761655918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1054245020587175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10540791567267935, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10539133845085855, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10537477038567729, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10535821146956613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10534166169496399, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1053251210543181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10530858954008408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10529206714472587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10527555386071567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10525904968053398, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10524255459666966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10522606860161977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10520959168788968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10519312384799291, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10517666507445142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10516021535979522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10514377469656262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10512734307730014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10511092049456243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10509450694091242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10507810240892115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10506170689116784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10504532038023984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10502894286873272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10501257434925, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10499621481440359, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10497986425681326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10496352266910698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10494719004392081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10493086637389884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10491455165169332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10489824586996435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10488194902138027, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10486566109861749, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10484938209436016, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10483311200130067, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10481685081213932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1048005985195845, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10478435511635237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10476812059516724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10475189494876133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1047356781698747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1047194702512555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10470327118565965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1046870809658511, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10467089958460161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10465472703469088, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10463856330890646, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1046224084000438, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10460626230090612, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10459012500430467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1045739965030583, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10455787678999376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10454176585794576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10452566369975662, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10450957030827657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10449348567636356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10447740979688333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10446134266270936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10444528426672293, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10442923460181298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10441319366087626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10439716143681722, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10438113792254794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10436512311098828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10434911699506579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10433311956771558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10431713082188061, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10430115075051132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10428517934656593, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1042692166030102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10425326251281751, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10423731706896892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10422138026445309, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1042054520922662, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10418953254541208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10417362161690212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1041577192997552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10414182558699789, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10412594047166415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1041100639467956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1040941960054413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10407833664065784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1040624858455093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1040466436130673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10403080993641091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10401498480862659, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10399916822280843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10398336017205786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10396756064948369, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1039517696482023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10393598716133742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10392021318202017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1039044477033891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10388869071859014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10387294222077666, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10385720220310926, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10384147065875603, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10382574758089234, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10381003296270096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10379432679737194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1037786290781026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10376293979809773, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10374725895056927, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10373158652873654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10371592252582605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10370026693507171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10368461974971457, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10366898096300298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10365335056819254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1036377285585461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1036221149273337, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10360650966783255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10359091277332719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10357532423710916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10355974405247743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10354417221273803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10352860871120402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10351305354119578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10349750669604081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10348196816907371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10346643795363625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10345091604307728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10343540243075268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10341989711002564, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10340440007426625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10338891131685175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10337343083116642, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10335795861060161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10334249464855581, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10332703893843437, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10331159147364982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10329615224762168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10328072125377645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10326529848554762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10324988393637576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10323447759970827, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10321907946899976, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10320368953771158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10318830779931215, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1031729342472768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10315756887508784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10314221167623444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10312686264421278, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10311152177252586, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10309618905468365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10308086448420296, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10306554805460753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10305023975942795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10303493959220168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10301964754647303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10300436361579313, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10298908779371997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10297382007381846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10295856044966017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10294330891482352, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10292806546289386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10291283008746316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10289760278213025, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10288238354050083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10286717235618714, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10285196922280836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10283677413399038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10282158708336582, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10280640806457397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10279123707126091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10277607409707941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10276091913568897, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10274577218075569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10273063322595251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10271550226495887, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10270037929146103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10268526429915185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10267015728173078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.102655058232904, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10263996714638424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10262488401589094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10260980883515011, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1025947415978944, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10257968229786298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10256463092880165, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10254958748446284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10253455195860545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10251952434499503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10250450463740364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1024894928296099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.102474488915399, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10245949288856251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10244450474289878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1024295244722124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10241455207031464, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10239958753102318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1023846308481622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10236968201556244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10235474102706095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10233980787650136, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10232488255773368, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10230996506461447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10229505539100664, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10228015353077943, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1022652594778087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10225037322597663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10223549476917176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10222062410128906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10220576121622989, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10219090610790192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10217605877021929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10216121919710244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10214638738247818, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10213156332027963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10211674700444634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10210193842892394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10208713758766468, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10207234447462697, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1020575590837755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10204278140908128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10202801144452164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10201324918408017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10199849462174669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10198374775151725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10196900856739433, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10195427706338638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10193955323350835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10192483707178124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10191012857223233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10189542772889515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1018807345358093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10186604898702077, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10185137107658156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10183670079854994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10182203814699037, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10180738311597333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10179273569957561, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10177809589188011, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10176346368697582, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10174883907895789, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10173422206192761, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10171961262999235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10170501077726556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10169041649786692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10167582978592203, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10166125063556271, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10164667904092672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10163211499615799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10161755849540652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1016030095328283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10158846810258537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1015739341988458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10155940781578376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10154488894757936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10153037758841875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10151587373249404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10150137737400346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10148688850715108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10147240712614704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1014579332252074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10144346679855423, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1014290078404156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1014145563450254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10140011230662355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10138567571945588, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10137124657777419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1013568248758361, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1013424106079053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10132800376825117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10131360435114924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10129921235088064, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10128482776173264, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10127045057799824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10125608079397636, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10124171840397168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10122736340229495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10121301578326251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10119867554119673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10118434267042566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10117001716528323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10115569902010926, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10114138822924924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10112708478705462, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10111278868788244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10109849992609561, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10108421849606296, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.10106994439215881, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10105567760876352, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10104141814026299, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10102716598104905, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10101292112551903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10099868356807626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10098445330312956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10097023032509365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10095601462838884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10094180620744121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1009276050566825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1009134111705501, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10089922454348717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1008850451699425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1008708730443705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10085670816123132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10084255051499073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10082840010012006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10081425691109644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10080012094240251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10078599218852652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1007718706439625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10075775630320977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10074364916077361, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10072954921116467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10071545644889925, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10070137086849923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10068729246449201, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10067322123141066, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10065915716379376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10064510025618534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10063105050313514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10061700789919834, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10060297243893566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10058894411691337, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10057492292770318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10056090886588241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10054690192603383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10053290210274574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10051890939061187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10050492378423141, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1004909452782091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10047697386715516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10046300954568518, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1004490523084203, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10043510214998698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10042115906501724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10040722304814848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10039329409402346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10037937219729054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10036545735260331, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10035154955462086, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1003376487980076, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10032375507743344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10030986838757358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10029598872310864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10028211607872456, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10026825044911276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10025439182896986, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10024054021299796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1002266955959044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10021285797240195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10019902733720866, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10018520368504792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10017138701064837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10015757730874404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.1001437745740743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10012997880138368, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10011618998542206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10010240812094463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10008863320271186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10007486522548947, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10006110418404839, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10004735007316488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10003360288762043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10001986262220178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.10000612927170083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09999240283091484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09997868329464617, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09996497065770249, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09995126491489659, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09993756606104659, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0999238740909756, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09991018899951216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09989651078148981, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09988283943174736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0998691749451288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09985551731648323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09984186654066492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09982822261253327, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09981458552695283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09980095527879343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09978733186292973, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0997737152742419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09976010550761485, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09974650255793885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09973290642010921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09971931708902629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09970573455959564, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09969215882672776, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09967858988533837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09966502773034816, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09965147235668301, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09963792375927365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09962438193305605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0996108468729712, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09959731857396506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09958379703098873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09957028223899822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09955677419295467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09954327288782411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09952977831857775, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09951629048019164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09950280936764695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09948933497592975, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09947586730003115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09946240633494723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09944895207567912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0994355045172326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09942206365461888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09940862948285381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09939520199695825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09938178119195805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09936836706288395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09935495960477159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09934155881266164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09932816468159965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09931477720663598, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09930139638282595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09928802220522993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09927465466891294, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09926129376894505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09924793950040114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09923459185836102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09922125083790931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09920791643413544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09919458864213389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09918126745700385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09916795287384933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09915464488777923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09914134349390731, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09912804868735213, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09911476046323703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09910147881669022, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09908820374284469, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09907493523683823, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09906167329381349, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09904841790891777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09903516907730331, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.099021926794127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09900869105455065, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09899546185374064, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09898223918686826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09896902304910961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09895581343564534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09894261034166096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09892941376234672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09891622369289758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09890304012851328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09888986306439813, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09887669249576131, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0988635284178167, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09885037082578274, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09883721971488273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09882407508034456, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09881093691740087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09879780522128895, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09878467998725074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09877156121053288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09875844888638663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09874534301006796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09873224357683746, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09871915058196039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09870606402070661, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09869298388835063, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0986799101801716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09866684289145332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09865378201748408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09864072755355695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09862767949496946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09861463783702389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09860160257502694, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09858857370429001, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09857555122012912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09856253511786468, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09854952539282191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09853652204033041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09852352505572441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09851053443434271, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09849755017152868, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09848457226263013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0984716007029995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09845863548799375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09844567661297428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09843272407330718, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09841977786436283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09840683798151639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09839390442014732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09838097717563961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09836805624338177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0983551416187668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0983422332971922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09832933127405992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09831643554477638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0983035461047525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0982906629494036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09827778607414948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09826491547441446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09825205114562717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09823919308322078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09822634128263286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09821349573930542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09820065644868482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09818782340622204, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09817499660737217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09816217604759499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09814936172235444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0981365536271191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09812375175736171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09811095610855954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09809816667619418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09808538345575164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09807260644272221, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09805983563260065, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09804707102088606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09803431260308178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0980215603746956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09800881433123967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09799607446823044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09798334078118863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09797061326563945, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09795789191711216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09794517673114063, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0979324677032629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09791976482902133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09790706810396253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09789437752363753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09788169308360144, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09786901477941391, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09785634260663874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09784367656084392, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09783101663760192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09781836283248922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09780571514108675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0977930735589797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09778043808175736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09776780870501339, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09775518542434562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09774256823535611, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09772995713365124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0977173521148415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09770475317454164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0976921603083706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09767957351195171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0976669927809121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09765441811088355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09764184949750171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09762928693640656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0976167304232423, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09760417995365711, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09759163552330356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0975790971278383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09756656476292212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09755403842421995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09754151810740096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09752900380813842, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09751649552210973, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09750399324499641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09749149697248412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0974790067002627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09746652242402609, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0974540441394723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09744157184230354, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09742910552822595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09741664519294999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09740419083219011, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0973917424416648, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09737930001709677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09736686355421273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09735443304874346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09734200849642381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09732958989299276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09731717723419327, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09730477051577247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09729236973348138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09727997488307523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09726758596031318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09725520296095853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09724282588077848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0972304547155444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09721808946103154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09720573011301938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09719337666729118, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09718102911963429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09716868746584015, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09715635170170411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09714402182302555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09713169782560784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09711937970525832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09710706745778828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09709476107901309, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09708246056475199, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09707016591082825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09705787711306903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09704559416730549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0970333170693728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09702104581510998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09700878040036005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09699652082096995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09698426707279051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09697201915167662, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09695977705348696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09694754077408414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09693531030933486, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09692308565510946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09691086680728231, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0968986537617318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09688644651434004, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09687424506099307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09686204939758089, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0968498595199974, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0968376754241402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0968254971059109, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09681332456121501, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09680115778596181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0967889967760645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09677684152744014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0967646920360095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09675254829769747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09674041030843249, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09672827806414706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09671615156077734, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09670403079426353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09669191576054936, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09667980645558263, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09666770287531487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09665560501570133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09664351287270127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09663142644227748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09661934572039683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0966072707030298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09659520138615066, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09658313776573753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09657107983777231, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09655902759824059, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09654698104313184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09653494016843914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09652290497015956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0965108754442937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09649885158684601, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0964868333938248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09647482086124183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09646281398511285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09645081276145728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09643881718629826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09642682725566262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09641484296558094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09640286431208753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09639089129122037, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09637892389902121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09636696213153545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0963550059848122, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09634305545490424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09633111053786811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.096319171229764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09630723752665575, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09629530942461088, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09628338691970069, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09627147000799996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09625955868558723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09624765294854484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09623575279295854, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09622385821491782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09621196921051595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0962000857758496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09618820790701932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09617633560012906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09616446885128663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0961526076566033, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09614075201219405, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09612890191417739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09611705735867557, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0961052183418143, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09609338485972303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09608155690853472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09606973448438595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09605791758341689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09604610620177131, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09603430033559657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09602249998104355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0960107051342668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09599891579142433, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09598713194867786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09597535360219253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09596358074813705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09595181338268385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09594005150200868, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09592829510229099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09591654417971374, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09590479873046333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09589305875072987, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09588132423670688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09586959518459137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09585787159058404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09584615345088887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09583444076171353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0958227335192692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09581103171977044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09579933535943545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09578764443448579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09577595894114657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09576427887564645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09575260423421754, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09574093501309533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09572927120851892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09571761281673083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.095705959833977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09569431225650696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09568267008057357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09567103330243319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09565940191834565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09564777592457424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0956361553173856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09562454009304998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09561293024784084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0956013257780353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09558972667991376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0955781329497601, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09556654458386156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09555496157850897, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09554338392999627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0955318116346211, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09552024468868435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09550868308849036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09549712683034688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09548557591056493, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0954740303254591, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09546249007134724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09545095514455063, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0954394255413939, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0954279012582051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0954163822913156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09540486863706012, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09539336029177684, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09538185725180712, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09537035951349589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09535886707319124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09534737992724471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09533589807201119, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09532442150384887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09531295021911923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09530148421418719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09529002348542086, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09527856802919181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09526711784187487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09525567291984811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09524423325949308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09523279885719443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09522136970934034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09520994581232207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09519852716253434, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09518711375637504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09517570559024545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09516430266055009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09515290496369676, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09514151249609651, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09513012525416373, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.095118743234316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09510736643297424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09509599484656259, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0950846284715084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09507326730424245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09506191134119853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09505056057881385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0950392150135288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09502787464178704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0950165394600354, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09500520946472403, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09499388465230627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09498256501923863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09497125056198093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09495994127699614, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09494863716075047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09493733820971338, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09492604442035744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09491475578915855, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09490347231259565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09489219398715104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09488092080931007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0948696527755614, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09485838988239677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09484713212631114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09483587950380272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09482463201137278, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09481338964552584, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09480215240276944, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09479092027961453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09477969327257506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09476847137816803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09475725459291387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09474604291333595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09473483633596085, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09472363485731826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09471243847394108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09470124718236526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09469006097912991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09467887986077725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09466770382385267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09465653286490469, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09464536698048487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09463420616714792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09462305042145165, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.094611899739957, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09460075411922798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09458961355583174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09457847804633848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09456734758732153, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09455622217535732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09454510180702524, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09453398647890791, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09452287618759098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09451177092966312, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09450067070171624, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09448957550034498, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09447848532214738, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09446740016372444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09445632002168014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09444524489262156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0944341747731589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09442310965990526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09441204954947688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09440099443849297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09438994432357593, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09437889920135105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09436785906844672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09435682392149418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09434579375712798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09433476857198551, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09432374836270715, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09431273312593641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09430172285831973, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09429071755650656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09427971721714931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09426872183690357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09425773141242769, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09424674594038317, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0942357654174344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09422478984024885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09421381920549687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09420285350985191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09419189274999024, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09418093692259123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09416998602433718, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09415904005191325, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09414809900200782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09413716287131196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09412623165651983, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09411530535432844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09410438396143794, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09409346747455123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09408255589037424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0940716492056158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09406074741698771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0940498505212047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09403895851498441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0940280713950474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09401718915811719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09400631180092012, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0939954393201856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0939845717126458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09397370897503589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09396285110409394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09395199809656088, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0939411499491806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09393030665869977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09391946822186806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09390863463543801, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09389780589616499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09388698200080736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09387616294612625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0938653487288857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09385453934585267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09384373479379692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09383293506949109, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09382214016971074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09381135009123426, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09380056483084284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09378978438532057, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09377900875145447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09376823792603421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09375747190585247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09374671068770475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09373595426838936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09372520264470743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09371445581346288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09370371377146262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09369297651551618, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09368224404243605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0936715163490375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0936607934321386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09365007528856026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09363936191512616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0936286533086629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09361794946599969, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09360725038396867, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09359655605940478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09358586648914571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09357518167003191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09356450159890677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09355382627261624, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0935431556880092, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09353248984193732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09352182873125497, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09351117235281932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09350052070349035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0934898737801307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0934792315796059, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09346859409878411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0934579613345364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0934473332837365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09343670994326084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09342609130998868, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09341547738080201, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09340486815258556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09339426362222678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09338366378661589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09337306864264582, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09336247818721218, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09335189241721341, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09334131132955056, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09333073492112753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09332016318885081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09330959612962969, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09329903374037611, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09328847601800477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09327792295943305, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09326737456158103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09325683082137148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09324629173572996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09323575730158454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09322522751586612, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0932147023755083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09320418187744725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09319366601862192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09318315479597393, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09317264820644748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09316214624698957, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0931516489145498, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09314115620608049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09313066811853649, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09312018464887549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09310970579405772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09309923155104607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09308876191680619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09307829688830623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09306783646251707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09305738063641217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09304692940696777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09303648277116258, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09302604072597807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09301560326839828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09300517039540986, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09299474210400215, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09298431839116704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09297389925389912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09296348468919556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09295307469405613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09294266926548315, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09293226840048172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0929218720960594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0929114803492264, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09290109315699555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09289071051638223, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09288033242440447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0928699588780828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09285958987444047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09284922541050322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09283886548329941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09282851008985997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09281815922721834, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09280781289241068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09279747108247566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09278713379445441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09277680102539074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09276647277233108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09275614903232429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09274582980242181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09273551507967766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0927252048611485, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09271489914389341, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09270459792497404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0926943012014546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09268400897040188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09267372122888518, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09266343797397629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09265315920274958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09264288491228195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09263261509965288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09262234976194421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09261208889624047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0926018324996286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09259158056919815, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09258133310204111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09257109009525202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09256085154592786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09255061745116822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09254038780807514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09253016261375316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09251994186530929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09250972555985305, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0924995136944965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09248930626635415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09247910327254297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09246890471018247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09245871057639458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09244852086830374, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09243833558303693, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09242815471772343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09241797826949526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09240780623548656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09239763861283425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09238747539867755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0923773165901581, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0923671621844202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09235701217861032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09234686656987767, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09233672535537372, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09232658853225244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0923164560976702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09230632804878594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09229620438276086, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09228608509675873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09227597018794571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0922658596534904, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0922557534905638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09224565169633935, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09223555426799293, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09222546120270275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09221537249764966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09220528815001666, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09219520815698934, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09218513251575558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09217506122350573, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09216499427743254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0921549316747312, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09214487341259923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09213481948823657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0921247698988456, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09211472464163097, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09210468371379982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09209464711256175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09208461483512856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09207458687871449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09206456324053625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09205454391781286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09204452890776565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09203451820761847, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09202451181459743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09201450972593099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09200451193885005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09199451845058784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09198452925837988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09197454435946417, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09196456375108102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09195458743047302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09194461539488516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09193464764156475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09192468416776148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09191472497072747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09190477004771695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09189481939598665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09188487301279558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09187493089540509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09186499304107894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09185505944708303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09184513011068571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09183520502915768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09182528419977184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0918153676198035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09180545528653028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09179554719723207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09178564334919105, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09177574373969177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09176584836602104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09175595722546802, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09174607031532407, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09173618763288292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0917263091754406, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0917164349402954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0917065649247479, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09169669912610098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0916868375416598, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09167698016873181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0916671270046267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09165727804665647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09164743329213537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09163759273837997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09162775638270906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09161792422244372, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09160809625490728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09159827247742533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09158845288732571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09157863748193856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09156882625859629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0915590192146334, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09154921634738687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09153941765419579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09152962313240152, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09151983277934764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09151004659237998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09150026456884669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.091490486706098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09148071300148654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09147094345236702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09146117805609653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09145141681003419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09144165971154158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09143190675798234, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09142215794672227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09141241327512958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09140267274057465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09139293634042987, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09138320407207005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09137347593287218, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09136375192021542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0913540320314811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09134431626405277, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09133460461531623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09132489708265937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09131519366347243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09130549435514766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09129579915507964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09128610806066509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09127642106930285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09126673817839405, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09125705938534191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09124738468755188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09123771408243157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09122804756739078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09121838513984139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09120872679719758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09119907253687562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09118942235629394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09117977625287318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09117013422403605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09116049626720753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09115086237981466, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09114123255928665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09113160680305495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09112198510855297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09111236747321645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09110275389448322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09109314436979318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09108353889658846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09107393747231324, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09106434009441386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09105474676033891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09104515746753894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09103557221346667, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09102599099557705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09101641381132695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09100684065817563, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09099727153358422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09098770643501612, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09097814535993667, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09096858830581364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09095903527011655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09094948625031722, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09093994124388959, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0909304002483096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09092086326105538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09091133027960707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09090180130144704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09089227632405962, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09088275534493123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09087323836155055, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09086372537140817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0908542163719968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09084471136081126, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0908352103353485, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0908257132931074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09081622023158908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09080673114829668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09079724604073537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09078776490641237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0907782877428371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0907688145475209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09075934531797722, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09074988005172167, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09074041874627176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0907309613991471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09072150800786946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09071205856996255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09070261308295213, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09069317154436612, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.09068373395173436, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0906743003025888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09066487059446333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09065544482489415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0906460229914191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09063660509157842, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0906271911229142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09061778108297054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09060837496929368, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09059897277943177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09058957451093506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09058018016135583, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09057078972824835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09056140320916889, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0905520206016758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09054264190332936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09053326711169189, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09052389622432783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09051452923880342, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0905051661526871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09049580696354924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09048645166896213, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09047710026650015, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09046775275373972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09045840912825923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09044906938763893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09043973352946116, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09043040155131035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09042107345077272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09041174922543667, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09040242887289238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09039311239073222, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0903837997765504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09037449102794311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09036518614250856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09035588511784699, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0903465879515605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09033729464125313, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09032800518453103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09031871957900227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09030943782227684, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09030015991196665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09029088584568566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09028161562104978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09027234923567674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0902630866871865, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09025382797320067, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09024457309134298, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09023532203923906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09022607481451651, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0902168314148048, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09020759183773541, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09019835608094179, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0901891241420592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09017989601872496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09017067170857823, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09016145120926021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09015223451841396, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09014302163368441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09013381255271846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09012460727316501, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0901154057926748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09010620810890044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09009701421949663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09008782412211985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09007863781442846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09006945529408279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09006027655874514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09005110160607963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0900419304337523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09003276303943111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09002359942078589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.09001443957548846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0900052835012124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08999613119563327, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08998698265642853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08997783788127749, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08996869686786135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08995955961386326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08995042611696819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08994129637486299, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08993217038523647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08992304814577921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08991392965418377, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08990481490814449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08989570390535764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0898865966435214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08987749312033574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08986839333350254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08985929728072556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08985020495971033, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08984111636816437, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08983203150379702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08982295036431942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08981387294744463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08980479925088758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08979572927236494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0897866630095954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0897776004602993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08976854162219902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0897594864930187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08975043507048426, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08974138735232358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08973234333626631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08972330302004394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08971426640138984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0897052334780391, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08969620424772884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08968717870819785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0896781568571867, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08966913869243805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08966012421169607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08965111341270697, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08964210629321867, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08963310285098096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08962410308374545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08961510698926553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08960611456529644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0895971258095952, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08958814071992066, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08957915929403346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08957018152969609, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08956120742467277, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08955223697672958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08954327018363444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08953430704315692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08952534755306857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08951639171114258, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08950743951515401, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08949849096287975, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08948954605209838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08948060478059038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08947166714613788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08946273314652492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08945380277953724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08944487604296246, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08943595293458981, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08942703345221047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08941811759361733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08940920535660501, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08940029673896996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08939139173851039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08938249035302623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08937359258031924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08936469841819289, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08935580786445253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08934692091690508, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08933803757335931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08932915783162591, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.089320281689517, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08931140914484675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08930254019543087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08929367483908696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08928481307363428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08927595489689397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0892671003066887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08925824930084308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08924940187718333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0892405580335376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08923171776773547, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08922288107760852, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08921404796098993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08920521841571474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0891963924396195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08918757003054277, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08917875118632465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08916993590480697, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08916112418383335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0891523160212491, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08914351141490125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08913471036263859, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08912591286231156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08911711891177233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08910832850887489, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08909954165147474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08909075833742927, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08908197856459754, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0890732023308402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08906442963401977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08905566047200034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08904689484264783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08903813274382975, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08902937417341533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08902061912927556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08901186760928302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0890031196113121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08899437513323875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08898563417294077, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08897689672829752, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08896816279719007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08895943237750122, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08895070546711543, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0889419820639188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08893326216579919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08892454577064605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0889158328763506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08890712348080562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08889841758190567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08888971517754696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08888101626562728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08887232084404625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.088863628910705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08885494046350634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0888462555003549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08883757401915678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08882889601781985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0888202214942536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08881155044636922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08880288287207946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08879421876929883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08878555813594344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08877690096993103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.088768247269181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08875959703161443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08875095025515402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08874230693772416, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08873366707725074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08872503067166146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08871639771888555, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08870776821685387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08869914216349908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08869051955675526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08868190039455828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08867328467484545, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08866467239555594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08865606355463038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0886474581500111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08863885617964203, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08863025764146874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08862166253343838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08861307085349977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08860448259960332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08859589776970107, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08858731636174665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08857873837369527, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08857016380350385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08856159264913087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08855302490853642, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08854446057968211, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08853589966053131, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08852734214904888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08851878804320137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08851023734095678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08850169004028488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08849314613915696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08848460563554589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08847606852742607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0884675348127737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08845900448956635, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08845047755578331, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08844195400940542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08843343384841505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08842491707079629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08841640367453463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08840789365761731, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08839938701803304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08839088375377215, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0883823838628266, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08837388734318972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08836539419285672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0883569044098241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08834841799209013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08833993493765455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08833145524451863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0883229789106853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08831450593415902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08830603631294578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0882975700450532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08828910712849034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08828064756126794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08827219134139822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08826373846689502, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08825528893577371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08824684274605114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08823839989574575, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08822996038287759, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08822152420546822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08821309136154071, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08820466184911971, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08819623566623137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08818781281090346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08817939328116522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08817097707504747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0881625641905825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08815415462580421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.088145748378748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08813734544745078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0881289458299511, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0881205495242888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08811215652850554, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0881037668406443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08809538045874968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08808699738086778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08807861760504615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08807024112933398, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0880618679517819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08805349807044209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08804513148336822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08803676818861553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08802840818424063, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08802005146830186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08801169803885885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0880033478939729, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08799500103170672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08798665745012457, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08797831714729219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08796998012127688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08796164637014736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08795331589197385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08794498868482817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0879366647467835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0879283440759146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08792002667029773, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08791171252801055, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08790340164713238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08789509402574387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08788678966192717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.087878488553766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08787019069934553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08786189609675235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08785360474407465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08784531663940202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08783703178082553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08782875016643779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08782047179433272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08781219666260597, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08780392476935442, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08779565611267656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08778739069067233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08777912850144315, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08777086954309184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08776261381372272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0877543613114416, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08774611203435573, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08773786598057386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0877296231482061, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08772138353536413, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08771314714016101, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08770491396071135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08769668399513103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08768845724153768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08768023369805006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08767201336278857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08766379623387503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08765558230943267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0876473715875862, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08763916406646173, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08763095974418691, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08762275861889073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0876145606887037, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08760636595175765, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08759817440618595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08758998605012336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0875818008817062, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08757361889907198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08756544010035987, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08755726448371035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08754909204726535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08754092278916828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0875327567075639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08752459380059839, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08751643406641943, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08750827750317615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0875001241090189, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08749197388209976, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08748382682057185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08747568292259002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08746754218631046, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0874594046098907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08745127019148967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08744313892926785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.087435010821387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08742688586601031, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08741876406130246, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08741064540542942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08740252989655864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08739441753285894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08738630831250059, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0873782022336552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0873700992944958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08736199949319683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08735390282793408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08734580929688483, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08733771889822768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08732963163014267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08732154749081111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08731346647841585, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0873053885911411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08729731382717237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08728924218469661, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08728117366190219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08727310825697883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0872650459681176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08725698679351102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08724893073135288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08724087777983852, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08723282793716444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08722478120152874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08721673757113069, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08720869704417107, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08720065961885201, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08719262529337689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08718459406595075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08717656593477956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08716854089807106, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08716051895403411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08715250010087905, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0871444843368176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0871364716600627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08712846206882878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0871204555613316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08711245213578822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08710445179041713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08709645452343813, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0870884603330724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08708046921754242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08707248117507209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08706449620388668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08705651430221263, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08704853546827793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0870405597003118, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08703258699654488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08702461735520904, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08701665077453771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08700868725276535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08700072678812801, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08699276937886295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08698481502320883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08697686371940563, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08696891546569459, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0869609702603184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08695302810152103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08694508898754778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08693715291664526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08692921988706143, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08692128989704552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08691336294484821, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08690543902872143, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08689751814691832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08688960029769358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08688168547930306, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0868737736900039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08686586492805472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08685795919171528, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08685005647924682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08684215678891175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08683426011897387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08682636646769827, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08681847583335137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0868105882142009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08680270360851579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08679482201456643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08678694343062449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08677906785496278, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08677119528585564, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0867633257215786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08675545916040844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08674759560062334, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08673973504050272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0867318774783273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08672402291237911, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08671617134094146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08670832276229895, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08670047717473754, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0866926345765444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08668479496600788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08667695834141792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0866691247010655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08666129404324291, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08665346636624388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08664564166836322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08663781994789718, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08663000120314318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08662218543240002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08661437263396765, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08660656280614741, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08659875594724191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08659095205555493, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08658315112939166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08657535316705843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08656755816686296, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08655976612711411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0865519770461222, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0865441909221986, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08653640775365608, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08652862753880862, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08652085027597148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08651307596346128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08650530459959568, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0864975361826938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08648977071107594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08648200818306365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08647424859697968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08646649195114826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08645873824389458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08645098747354528, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0864432396384282, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08643549473687238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08642775276720821, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0864200137277672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08641227761688226, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08640454443288742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08639681417411797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08638908683891054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08638136242560286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08637364093253398, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08636592235804424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08635820670047518, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08635049395816946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08634278412947116, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08633507721272544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08632737320627885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08631967210847907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08631197391767496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08630427863221678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08629658625045583, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08628889677074476, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08628121019143746, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08627352651088899, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0862658457274556, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08625816783949483, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08625049284536544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08624282074342739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08623515153204185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08622748520957128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08621982177437924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08621216122483058, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08620450355929134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08619684877612883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08618919687371154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08618154785040914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08617390170459253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08616625843463381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08615861803890637, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08615098051578467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08614334586364451, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08613571408086276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08612808516581769, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08612045911688858, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08611283593245592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08610521561090156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08609759815060843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0860899835499607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08608237180734371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08607476292114402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08606715688974931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08605955371154861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08605195338493203, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08604435590829085, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08603676128001764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08602916949850606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.086021580562151, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08601399446934863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08600641121849613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08599883080799194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08599125323623577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08598367850162839, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08597610660257185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08596853753746932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08596097130472513, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08595340790274483, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08594584732993521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08593828958470406, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08593073466546051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08592318257061488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08591563329857843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08590808684776391, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08590054321658495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08589300240345664, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08588546440679497, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08587792922501725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08587039685654192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08586286729978856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08585534055317794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08584781661513206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08584029548407397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08583277715842785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0858252616366193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08581774891707478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08581023899822199, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08580273187848989, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08579522755630853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08578772603010908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08578022729832392, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08577273135938655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08576523821173163, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08575774785379503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08575026028401365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08574277550082562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08573529350267023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08572781428798783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08572033785522004, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08571286420280948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0857053933292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08569792523283669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08569045991216558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0856829973656339, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08567553759169017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08566808058878382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08566062635536559, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08565317488988725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08564572619080184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08563828025656335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08563083708562699, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08562339667644922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08561595902748739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0856085241372002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08560109200404732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08559366262648967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0855862360029892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08557881213200907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08557139101201351, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08556397264146785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08555655701883862, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08554914414259344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08554173401120098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08553432662313115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08552692197685487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08551952007084426, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08551212090357252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08550472447351394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08549733077914397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08548993981893915, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08548255159137716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08547516609493674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08546778332809778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08546040328934129, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08545302597714931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0854456513900051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08543827952639291, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08543091038479818, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0854235439637075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0854161802616084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08540881927698961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08540146100834099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08539410545415348, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08538675261291902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08537940248313083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08537205506328309, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08536471035187108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08535736834739127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08535002904834114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08534269245321931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08533535856052538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08532802736876023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08532069887642572, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08531337308202473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08530604998406141, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08529872958104082, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08529141187146921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08528409685385392, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08527678452670329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0852694748885268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.085262167937835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08525486367313956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08524756209295319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08524026319578967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08523296698016389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08522567344459174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08521838258759035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08521109440767775, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08520380890337315, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08519652607319678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08518924591566997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08518196842931514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08517469361265569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0851674214642162, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08516015198252228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08515288516610058, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08514562101347888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08513835952318585, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0851311006937515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08512384452370678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08511659101158353, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08510934015591491, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08510209195523499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08509484640807899, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08508760351298311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08508036326848467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.085073125673122, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08506589072543447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0850586584239626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08505142876724786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08504420175383286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08503697738226113, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08502975565107747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08502253655882744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08501532010405798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08500810628531677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08500089510115273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08499368655011577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08498648063075681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08497927734162797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08497207668128212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08496487864827347, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08495768324115707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08495049045848913, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08494330029882688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0849361127607285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08492892784275335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08492174554346169, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08491456586141495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08490738879517541, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08490021434330662, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08489304250437295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08488587327693992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08487870665957407, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08487154265084292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08486438124931513, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08485722245356024, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08485006626214886, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08484291267365274, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08483576168664453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08482861329969793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08482146751138774, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08481432432028964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08480718372498051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08480004572403808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0847929103160412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08478577749956973, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0847786472732045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08477151963552743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0847643945851214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08475727212057035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08475015224045919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08474303494337382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0847359202279013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08472880809262948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08472169853614737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08471459155704505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08470748715391344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0847003853253445, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08469328606993128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08468618938626789, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08467909527294927, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08467200372857145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0846649147517315, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08465782834102742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08465074449505826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08464366321242407, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08463658449172587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08462950833156574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08462243473054668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08461536368727277, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.084608295200349, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08460122926838139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08459416588997701, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08458710506374387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08458004678829092, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08457299106222824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08456593788416678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08455888725271851, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08455183916649643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08454479362411454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08453775062418772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08453071016533195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08452367224616412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08451663686530214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08450960402136497, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08450257371297243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08449554593874534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08448852069730564, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08448149798727604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08447447780728042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08446746015594353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08446044503189111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08445343243374995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08444642236014772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08443941480971312, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08443240978107577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08442540727286636, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08441840728371644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0844114098122587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08440441485712653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08439742241695462, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08439043249037832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08438344507603414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08437646017255955, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08436947777859288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0843624978927735, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0843555205137418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.084348545640139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08434157327060732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08433460340379006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0843276360383314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0843206711728764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08431370880607118, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08430674893656281, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08429979156299933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0842928366840297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08428588429830382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08427893440447255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08427198700118782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08426504208710232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0842580996608699, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08425115972114519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08424422226658382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08423728729584248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08423035480757864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08422342480045081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08421649727311842, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08420957222424198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0842026496524827, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0841957295565029, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08418881193496586, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0841818967865357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08417498410987755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08416807390365745, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0841611661665425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08415426089720054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08414735809430045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0841404577565121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08413355988250626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08412666447095457, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08411977152052975, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08411288102990522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08410599299775565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08409910742275635, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08409222430358376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08408534363891522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08407846542742879, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08407158966780386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08406471635872036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08405784549885936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08405097708690284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08404411112153362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08403724760143558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08403038652529343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08402352789179279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08401667169962027, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08400981794746333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08400296663401045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08399611775795095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08398927131797507, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08398242731277407, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08397558574104001, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08396874660146597, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08396190989274578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08395507561357439, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08394824376264753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08394141433866195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0839345873403152, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08392776276630583, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08392094061533326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08391412088609783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08390730357730082, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08390048868764435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08389367621583152, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08388686616056631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08388005852055365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08387325329449927, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08386645048110992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08385965007909316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08385285208715755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08384605650401254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0838392633283684, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08383247255893635, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08382568419442858, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08381889823355802, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08381211467503871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08380533351758537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08379855475991378, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08379177840074054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08378500443878326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08377823287276023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08377146370139074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08376469692339518, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08375793253749447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0837511705424107, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08374441093686669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08373765371958625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08373089888929407, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08372414644471562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08371739638457747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08371064870760685, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08370390341253202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08369716049808207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08369041996298705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08368368180597778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08367694602578599, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08367021262114441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08366348159078654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08365675293344677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08365002664786042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08364330273276366, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08363658118689352, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08362986200898798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08362314519778576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08361643075202661, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08360971867045112, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08360300895180069, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08359630159481762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08358959659824515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08358289396082727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08357619368130903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08356949575843613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08356280019095533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08355610697761409, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08354941611716091, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08354272760834505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08353604144991669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08352935764062681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08352267617922733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.083515997064471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08350932029511145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08350264586990319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0834959737876015, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08348930404696271, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08348263664674381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08347597158570273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08346930886259832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0834626484761902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08345599042523894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08344933470850587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08344268132475324, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08343603027274417, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08342938155124253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0834227351590132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08341609109482186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08340944935743491, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08340280994561985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08339617285814477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08338953809377884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.083382905651292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08337627552945494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08336964772703928, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0833630222428176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08335639907556307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08334977822404994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08334315968705328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08333654346334883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08332992955171337, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08332331795092444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08331670865976044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08331010167700056, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08330349700142496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08329689463181444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0832902945669509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08328369680561684, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08327710134659579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08327050818867195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08326391733063045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08325732877125726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0832507425093392, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08324415854366386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08323757687301969, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08323099749619606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08322442041198301, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08321784561917156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0832112731165535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08320470290292142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08319813497706888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08319156933779001, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08318500598388008, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08317844491413492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08317188612735137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08316532962232702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08315877539786026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0831522234527504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08314567378579758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08313912639580252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08313258128156709, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08312603844189377, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08311949787558598, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0831129595814479, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0831064235582845, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08309988980490168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08309335832010607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08308682910270516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08308030215150719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08307377746532134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08306725504295744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08306073488322632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08305421698493948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08304770134690932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08304118796794906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08303467684687259, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0830281679824948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08302166137363134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08301515701909852, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08300865491771375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08300215506829495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08299565746966102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08298916212063163, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08298266902002728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08297617816666925, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08296968955937958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08296320319698125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0829567190782979, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.082950237202154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08294375756737497, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08293728017278683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08293080501721654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0829243320994918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08291786141844112, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08291139297289378, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08290492676167993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08289846278363051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08289200103757721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08288554152235254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0828790842367898, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08287262917972305, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08286617634998725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08285972574641808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08285327736785196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08284683121312628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08284038728107901, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08283394557054906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08282750608037605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08282106880940045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08281463375646349, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08280820092040724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08280177030007438, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08279534189430864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08278891570195433, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08278249172185669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08277606995286156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08276965039381584, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08276323304356695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0827568179009632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08275040496485374, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0827439942340884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08273758570751788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08273117938399356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08272477526236771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08271837334149335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0827119736202242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08270557609741484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08269918077192062, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08269278764259762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08268639670830276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08268000796789368, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08267362142022888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0826672370641675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08266085489856953, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08265447492229579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08264809713420777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08264172153316779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08263534811803892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.082628976887685, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08262260784097068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08261624097676133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08260987629392307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08260351379132287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08259715346782842, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0825907953223081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08258443935363126, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0825780855606678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08257173394228848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08256538449736484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08255903722476915, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08255269212337446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08254634919205461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08254000842968401, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08253366983513824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08252733340729318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0825209991450258, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0825146670472136, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08250833711273503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08250200934046917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08249568372929592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0824893602780959, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0824830389857505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08247671985114191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08247040287315295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08246408805066736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0824577753825695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08245146486774449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08244515650507836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08243885029345767, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08243254623176988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08242624431890311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08241994455374634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08241364693518916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08240735146212202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08240105813343608, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08239476694802327, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08238847790477616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08238219100258819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08237590624035354, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08236962361696702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08236334313132432, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08235706478232178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08235078856885653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08234451448982638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08233824254413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08233197273066672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08232570504833653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08231943949604036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08231317607267966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08230691477715683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08230065560837478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08229439856523739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08228814364664916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08228189085151526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08227564017874162, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0822693916272351, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08226314519590305, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0822569008836537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08225065868939585, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08224441861203922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08223818065049421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08223194480367191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08222571107048406, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08221947944984329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08221324994066294, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08220702254185695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08220079725234009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08219457407102787, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08218835299683643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08218213402868271, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08217591716548442, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08216970240615981, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08216348974962814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08215727919480911, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0821510707406233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08214486438599206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0821386601298372, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08213245797108155, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08212625790864854, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08212005994146232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08211386406844774, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08210767028853037, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08210147860063653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08209528900369326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08208910149662832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08208291607837008, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08207673274784777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08207055150399131, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08206437234573123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0820581952719989, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08205202028172628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08204584737384615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08203967654729202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08203350780099795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08202734113389891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08202117654493041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08201501403302877, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08200885359713102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0820026952361748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08199653894909861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08199038473484153, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08198423259234339, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08197808252054471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08197193451838683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0819657885848116, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08195964471876167, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08195350291918051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08194736318501207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08194122551520117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08193508990869325, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08192895636443447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08192282488137169, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08191669545845254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08191056809462521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0819044427888387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08189831954004262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08189219834718747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08188607920922418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08187996212510454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.081873847093781, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08186773411420678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08186162318533559, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08185551430612206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08184940747552144, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08184330269248957, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0818371999559831, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08183109926495939, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08182500061837641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08181890401519279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08181280945436799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08180671693486202, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08180062645563571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0817945380156505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08178845161386848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08178236724925254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08177628492076604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08177020462737336, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08176412636803933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08175805014172947, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08175197594741009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08174590378404809, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08173983365061108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08173376554606746, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08172769946938613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08172163541953677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08171557339548977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08170951339621606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08170345542068745, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08169739946787631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08169134553675575, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08168529362629945, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08167924373548183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08167319586327804, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08166715000866384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0816611061706157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08165506434811072, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08164902454012675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08164298674564226, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08163695096363638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.081630917193089, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08162488543298052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08161885568229224, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08161282794000592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08160680220510415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08160077847657002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08159475675338748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08158873703454099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08158271931901577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08157670360579769, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08157068989387331, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08156467818222983, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08155866846985506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08155266075573758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0815466550388666, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08154065131823197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08153464959282421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08152864986163454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08152265212365478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0815166563778775, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08151066262329584, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08150467085890366, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08149868108369546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08149269329666645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08148670749681243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08148072368312984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08147474185461588, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08146876201026838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08146278414908573, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08145680827006709, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08145083437221226, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08144486245452166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08143889251599638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08143292455563814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08142695857244936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08142099456543309, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0814150325335931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08140907247593361, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08140311439145981, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08139715827917723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08139120413809228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08138525196721187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08137930176554363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08137335353209589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08136740726587752, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08136146296589805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0813555206311678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08134958026069758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08134364185349885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0813377054085839, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08133177092496545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08132583840165694, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08131990783767247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08131397923202686, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08130805258373541, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08130212789181422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08129620515527991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08129028437314982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08128436554444193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08127844866817477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08127253374336767, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08126662076904047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08126070974421369, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08125480066790847, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08124889353914669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08124298835695073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08123708512034367, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08123118382834922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08122528447999174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08121938707429628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08121349161028835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08120759808699429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08120170650344101, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08119581685865594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08118992915166734, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08118404338150396, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08117815954719526, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0811722776477713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08116639768226275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08116051964970092, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08115464354911779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08114876937954595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0811428971400186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08113702682956965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08113115844723344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0811252919920452, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08111942746304064, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08111356485925603, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0811077041797284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0811018454234954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08109598858959524, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08109013367706674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08108428068494945, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08107842961228344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08107258045810946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08106673322146883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08106088790140362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08105504449695629, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08104920300717018, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08104336343108909, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08103752576775747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08103169001622043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08102585617552369, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0810200242447135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0810141942228369, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08100836610894137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08100253990207514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08099671560128692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08099089320562622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08098507271414306, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08097925412588797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08097343743991232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.080967622655268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08096180977100739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08095599878618366, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08095018969985056, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0809443825110623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08093857721887392, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08093277382234093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08092697232051949, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08092117271246639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08091537499723891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08090957917389524, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08090378524149382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08089799319909391, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08089220304575534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08088641478053848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08088062840250446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08087484391071482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08086906130423185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08086328058211842, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08085750174343795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08085172478725451, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0808459497126328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.080840176518638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08083440520433612, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08082863576879351, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08082286821107727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08081710253025515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0808113387253954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08080557679556682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08079981673983898, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08079405855728196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08078830224696641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08078254780796362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08077679523934549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08077104454018447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08076529570955364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08075954874652665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08075380365017783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08074806041958196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0807423190538146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08073657955195171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08073084191306999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08072510613624667, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08071937222055962, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08071364016508721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0807079099689085, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08070218163110308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0806964551507512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08069073052693365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08068500775873177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08067928684522757, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08067356778550365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08066785057864317, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08066213522372985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08065642171984798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08065071006608253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08064500026151908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08063929230524358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08063358619634285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08062788193390412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08062217951701521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08061647894476463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08061078021624132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.080605083330535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08059938828673577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08059369508393444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08058800372122238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08058231419769155, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08057662651243443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0805709406645442, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08056525665311441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08055957447723952, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08055389413601421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08054821562853405, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08054253895389492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08053686411119344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08053119109952687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08052551991799285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08051985056568972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08051418304171637, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08050851734517235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0805028534751576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0804971914307728, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08049153121111914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08048587281529838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08048021624241292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08047456149156565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08046890856186004, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08046325745240013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08045760816229064, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.08045196069063679, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08044631503654429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0804406711991195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08043502917746936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08042938897070145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0804237505779237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08041811399824478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08041247923077395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08040684627462094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08040121512889602, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08039558579271025, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08038995826517495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08038433254540228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08037870863250471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08037308652559554, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08036746622378838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08036184772619762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08035623103193812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08035061614012529, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08034500304987509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08033939176030408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08033378227052938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08032817457966875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08032256868684032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08031696459116289, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08031136229175592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08030576178773921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08030016307823329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08029456616235923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08028897103923859, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08028337770799351, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08027778616774676, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08027219641762154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08026660845674174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08026102228423175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08025543789921645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08024985530082136, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08024427448817256, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0802386954603966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08023311821662071, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08022754275597255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08022196907758046, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08021639718057318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08021082706408009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08020525872723117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08019969216915689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08019412738898822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08018856438585681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08018300315889475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08017744370723474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08017188603000995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08016633012635425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08016077599540193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08015522363628785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08014967304814746, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08014412423011671, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08013857718133212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0801330319009308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08012748838805028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08012194664182878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.080116406661405, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08011086844591812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08010533199450803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08009979730631499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08009426438047992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08008873321614424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08008320381244989, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08007767616853938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08007215028355576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08006662615664259, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08006110378694413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08005558317360491, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08005006431577019, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08004454721258573, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08003903186319782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08003351826675328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08002800642239946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08002249632928432, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08001698798655625, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08001148139336427, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08000597654885785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.08000047345218714, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07999497210250262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07998947249895544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07998397464069727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07997847852688036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07997298415665736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07996749152918158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0799620006436068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07995651149908738, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07995102409477808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07994553842983443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07994005450341227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07993457231466808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07992909186275883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0799236131468421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07991813616607585, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07991266091961874, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07990718740662982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07990171562626876, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07989624557769569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0798907772600713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07988531067255687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07987984581431412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07987438268450525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07986892128229321, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07986346160684121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07985800365731312, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07985254743287332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07984709293268671, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07984164015591873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07983618910173537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.079830739769303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07982529215778868, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07981984626635998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0798144020941848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07980895964043185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07980351890427011, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07979807988486923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07979264258139934, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07978720699303106, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07978177311893558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07977634095828459, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07977091051025029, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07976548177400533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07976005474872308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07975462943357721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.079749205827742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0797437839303923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07973836374070335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07973294525785105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07972752848101161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07972211340936204, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07971670004207963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07971128837834232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0797058784173284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0797004701582169, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07969506360018717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07968965874241922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07968425558409346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07967885412439087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07967345436249289, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07966805629758154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07966265992883935, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07965726525544929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07965187227659484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07964648099146006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07964109139922958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07963570349908833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07963031729022191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07962493277181641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07961954994305832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07961416880313482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07960878935123347, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07960341158654229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07959803550824997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07959266111554557, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07958728840761871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07958191738365954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07957654804285857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07957118038440708, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07956581440749663, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07956045011131932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07955508749506779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07954972655793519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07954436729911522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07953900971780195, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07953365381319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07952829958447465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07952294703085139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07951759615151643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07951224694566641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07950689941249851, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07950155355121034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07949620936100006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07949086684106635, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07948552599060822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07948018680882547, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07947484929491809, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07946951344808681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07946417926753269, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07945884675245747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07945351590206316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07944818671555236, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0794428591921283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07943753333099449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07943220913135507, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0794268865924146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07942156571337819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07941624649345137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07941092893184032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07940561302775154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07940029878039205, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07939498618896948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07938967525269183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07938436597076762, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07937905834240587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07937375236681608, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07936844804320828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.079363145370793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07935784434878115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07935254497638418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0793472472528141, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07934195117728336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07933665674900485, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07933136396719198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0793260728310587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07932078333981936, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0793154954926889, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07931020928888263, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0793049247276164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07929964180810652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07929436052956991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07928908089122372, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07928380289228579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07927852653197448, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07927325180950844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07926797872410692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07926270727498966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07925743746137683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07925216928248915, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07924690273754768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07924163782577424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07923637454639076, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07923111289861993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07922585288168488, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07922059449480906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07921533773721653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07921008260813191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07920482910678009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0791995772323866, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07919432698417733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07918907836137876, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07918383136321777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07917858598892176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07917334223771857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07916810010883651, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07916285960150443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07915762071495162, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07915238344840779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07914714780110318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07914191377226856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07913668136113496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07913145056693416, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07912622138889829, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07912099382625985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07911576787825196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0791105435441082, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07910532082306254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0791000997143494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07909488021720383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07908966233086125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07908444605455749, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07907923138752893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07907401832901242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07906880687824523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07906359703446517, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07905838879691043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07905318216481971, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07904797713743227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07904277371398764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07903757189372596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07903237167588785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07902717305971425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07902197604444676, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07901678062932727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07901158681359824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07900639459650255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07900120397728362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07899601495518521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07899082752945166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07898564169932765, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07898045746405843, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07897527482288969, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07897009377506757, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07896491431983865, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07895973645644998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07895456018414908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07894938550218396, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07894421240980307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07893904090625532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07893387099078997, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07892870266265697, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07892353592110653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07891837076538938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07891320719475675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07890804520846026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07890288480575205, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07889772598588471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0788925687481112, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07888741309168508, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07888225901586021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07887710651989104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07887195560303238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07886680626453954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07886165850366832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07885651231967487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0788513677118159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07884622467934854, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07884108322153033, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07883594333761931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07883080502687395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07882566828855317, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07882053312191643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07881539952622348, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07881026750073461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07880513704471065, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07880000815741267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07879488083810235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07878975508604177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07878463090049355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07877950828072057, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0787743872259863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07876926773555464, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07876414980868988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07875903344465689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07875391864272081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0787488054021473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07874369372220258, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07873858360215318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07873347504126604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0787283680388087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07872326259404903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07871815870625545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07871305637469665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07870795559864194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07870285637736094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07869775871012388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07869266259620128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0786875680348641, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07868247502538385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07867738356703242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0786722936590822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07866720530080588, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07866211849147675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07865703323036845, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07865194951675512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07864686734991123, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0786417867291119, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07863670765363247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07863163012274876, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07862655413573719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07862147969187443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07861640679043766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07861133543070455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07860626561195314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07860119733346187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07859613059450977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07859106539437614, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0785860017323408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07858093960768404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0785758790196865, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0785708199676293, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07856576245079394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07856070646846255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07855565201991738, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07855059910444138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07854554772131778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07854049786983038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07853544954926328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07853040275890111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07852535749802883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07852031376593194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07851527156189626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07851023088520818, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07850519173515445, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07850015411102218, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07849511801209902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07849008343767301, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07848505038703264, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07848001885946675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07847498885426471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07846996037071627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07846493340811159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07845990796574134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07845488404289655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07844986163886866, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07844484075294954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07843982138443158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07843480353260748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07842978719677052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07842477237621413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07841975907023248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07841474727811999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0784097369991715, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07840472823268244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07839972097794837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07839471523426553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07838971100093055, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07838470827724028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07837970706249232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07837470735598441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07836970915701488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07836471246488233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07835971727888599, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07835472359832532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0783497314225003, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07834474075071138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07833975158225921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07833476391644514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07832977775257075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0783247930899381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07831980992784973, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07831482826560847, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07830984810251766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07830486943788104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0782998922710028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07829491660118751, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07828994242774007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07828496974996595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.078279998567171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07827502887866142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07827006068374388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07826509398172549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07826012877191371, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07825516505361646, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07825020282614203, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07824524208879917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07824028284089704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0782353250817452, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07823036881065365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07822541402693275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07822046072989329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07821550891884654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07821055859310412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07820560975197806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07820066239478084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781957165208252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07819077212942463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07818582921989267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781808877915435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781759478436916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07817100937565184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07816607238673967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07816113687627073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07815620284356121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07815127028792768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07814633920868704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781414096051568, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781364814766546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07813155482249877, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0781266296420078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07812170593450073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07811678369929703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07811186293571645, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07810694364307925, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07810202582070609, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07809710946791792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0780921945840363, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07808728116838302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0780823692202803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07807745873905093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07807254972401782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07806764217450449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07806273608983483, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07805783146933319, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07805292831232409, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07804802661813269, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07804312638608453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0780382276155054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07803333030572164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07802843445605986, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07802354006584732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07801864713441135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0780137556610799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07800886564518125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07800397708604413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07799908998299764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07799420433537113, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07798932014249467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07798443740369845, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07797955611831323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07797467628567004, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07796979790510035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07796492097593605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07796004549750946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0779551714691532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07795029889020041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07794542775998455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07794055807783944, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0779356898430994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07793082305509902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07792595771317341, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07792109381665795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07791623136488855, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07791137035720146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07790651079293325, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.077901652671421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07789679599200208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07789194075401436, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.077887086956796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07788223459968562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0778773836820222, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07787253420314515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07786768616239419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07786283955910951, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07785799439263172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07785315066230174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07784830836746084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07784346750745089, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07783862808161386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07783379008929237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0778289535298293, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07782411840256792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0778192847068519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07781445244202533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07780962160743267, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0778047922024187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07779996422632875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07779513767850835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07779031255830357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07778548886506081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0777806665981268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07777584575684875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07777102634057419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07776620834865104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07776139178042767, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07775657663525279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07775176291247546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07774695061144515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07774213973151178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07773733027202559, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07773252223233718, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07772771561179756, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07772291040975816, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07771810662557078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07771330425858755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07770850330816102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07770370377364415, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07769890565439022, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07769410894975293, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07768931365908639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07768451978174504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0776797273170837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07767493626445757, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0776701466232223, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07766535839273384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07766057157234856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07765578616142318, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07765100215931486, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07764621956538106, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07764143837897963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07763665859946892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07763188022620744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07762710325855429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07762232769586878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07761755353751075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07761278078284035, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07760800943121801, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07760323948200469, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07759847093456161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07759370378825046, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07758893804243328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07758417369647243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07757941074973068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07757464920157119, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07756988905135745, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07756513029845347, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07756037294222334, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07755561698203187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07755086241724399, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07754610924722509, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07754135747134097, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07753660708895775, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07753185809944192, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07752711050216042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07752236429648042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07751761948176958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07751287605739592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07750813402272777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07750339337713388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07749865411998336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07749391625064568, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07748917976849075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07748444467288863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07747971096321009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07747497863882599, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07747024769910758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07746551814342677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07746078997115545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07745606318166606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07745133777433147, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07744661374852478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07744189110361958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07743716983898968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0774324499540094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07742773144805343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07742301432049668, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07741829857071458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07741358419808281, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07740887120197748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07740415958177504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07739944933685235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07739474046658655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07739003297035525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07738532684753636, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07738062209750807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0773759187196492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07737121671333863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07736651607795574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07736181681288026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07735711891749238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07735242239117246, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0773477272333014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07734303344326028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07733834102043075, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07733364996419463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07732896027393428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07732427194903228, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07731958498887162, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07731489939283558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07731021516030799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07730553229067286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07730085078331461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07729617063761803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07729149185296827, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07728681442875082, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07728213836435158, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07727746365915673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07727279031255291, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07726811832392695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07726344769266623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07725877841815837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0772541104997914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07724944393695364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07724477872903389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0772401148754211, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07723545237550479, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07723079122867477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07722613143432114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07722147299183434, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0772168159006053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07721216016002523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07720750576948567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07720285272837853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07719820103609605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07719355069203096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0771889016955761, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07718425404612489, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07717960774307094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07717496278580833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07717031917373147, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07716567690623505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07716103598271419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07715639640256429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0771517581651812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.077147121269961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07714248571630021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07713785150359567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0771332186312446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07712858709864456, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07712395690519337, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07711932805028933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07711470053333104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07711007435371735, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07710544951084766, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07710082600412159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07709620383293905, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07709158299670046, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07708696349480644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0770823453266581, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07707772849165671, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0770731129892041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07706849881870227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07706388597955366, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07705927447116104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07705466429292747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07705005544425647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07704544792455181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07704084173321765, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07703623686965853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07703163333327914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07702703112348477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07702243023968093, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07701783068127346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07701323244766863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0770086355382729, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07700403995249326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0769994456897369, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07699485274941145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07699026113092478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07698567083368515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07698108185710126, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07697649420058197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07697190786353664, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07696732284537487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07696273914550661, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07695815676334225, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0769535756982924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07694899594976806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07694441751718056, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07693984039994162, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07693526459746321, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07693069010915772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07692611693443784, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07692154507271658, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07691697452340733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07691240528592379, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07690783735968006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07690327074409045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07689870543856976, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07689414144253302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07688957875539557, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0768850173765733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07688045730548214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07687589854153853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07687134108415927, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07686678493276139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07686223008676235, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07685767654557983, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.076853124308632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07684857337533726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07684402374511433, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07683947541738242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07683492839156081, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07683038266706933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07682583824332806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07682129511975744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07681675329577824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07681221277081154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07680767354427878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0768031356156017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07679859898420241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07679406364950336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07678952961092726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07678499686789719, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07678046541983669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07677593526616931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07677140640631934, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07676687883971105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07676235256576926, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.076757827583919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07675330389358569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07674878149419509, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0767442603851732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07673974056594647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07673522203594163, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07673070479458571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07672618884130607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07672167417553043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07671716079668682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07671264870420365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07670813789750959, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07670362837603363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07669912013920513, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07669461318645379, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07669010751720956, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07668560313090278, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07668110002696418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0766765982048247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07667209766391558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07666759840366849, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07666310042351542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07665860372288863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0766541083012207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07664961415794463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07664512129249361, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07664062970430123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07663613939280145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0766316503574284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07662716259761675, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07662267611280126, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07661819090241723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07661370696590009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07660922430268577, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07660474291221038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0766002627939104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0765957839472227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07659130637158436, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07658683006643283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07658235503120593, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07657788126534172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07657340876827866, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0765689375394554, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07656446757831109, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07655999888428511, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07655553145681707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07655106529534708, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07654660039931539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07654213676816272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07653767440133002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07653321329825863, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07652875345839014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07652429488116644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07651983756602983, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07651538151242282, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07651092671978836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07650647318756963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07650202091521013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07649756990215376, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0764931201478446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07648867165172717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07648422441324619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07647977843184688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07647533370697455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07647089023807495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07646644802459421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07646200706597861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07645756736167489, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07645312891112994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0764486917137912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07644425576910628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07643982107652308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07643538763548978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07643095544545507, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07642652450586776, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0764220948161771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07641766637583253, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07641323918428393, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07640881324098137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07640438854537536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07639996509691657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07639554289505616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07639112193924545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07638670222893616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07638228376358028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07637786654263012, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07637345056553833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07636903583175786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07636462234074187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07636021009194398, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07635579908481809, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07635138931881824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07634698079339912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07634257350801535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0763381674621221, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07633376265517482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07632935908662915, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0763249567559412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07632055566256722, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07631615580596393, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07631175718558829, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0763073598008975, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0763029636513492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07629856873640117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0762941750555117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07628978260813919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0762853913937425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07628100141178076, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07627661266171332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0762722251429999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07626783885510052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07626345379747551, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07625906996958552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07625468737089146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07625030600085463, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07624592585893646, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07624154694459892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07623716925730412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07623279279651453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0762284175616929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07622404355230224, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07621967076780604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0762152992076679, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07621092887135177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.076206559758322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07620219186804314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07619782519998006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07619345975359797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07618909552836227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07618473252373892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07618037073919384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07617601017419355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07617165082820467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0761672927006942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07616293579112941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07615858009897798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07615422562370776, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07614987236478692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07614552032168397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0761411694938677, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07613681988080725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07613247148197197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07612812429683155, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.076123778324856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07611943356551558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07611509001828094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07611074768262287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07610640655801261, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07610206664392166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07609772793982182, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07609339044518502, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07608905415948382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07608471908219074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07608038521277888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07607605255072139, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07607172109549193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07606739084656428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07606306180341259, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07605873396551135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07605440733233527, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07605008190335942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07604575767805913, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07604143465591001, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07603711283638798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07603279221896928, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07602847280313041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07602415458834816, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07601983757409965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07601552175986229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07601120714511367, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07600689372933186, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07600258151199517, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07599827049258208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07599396067057146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07598965204544247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07598534461667458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0759810383837475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07597673334614127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07597242950333621, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07596812685481291, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07596382540005227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07595952513853552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0759552260697441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0759509281931598, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07594663150826465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07594233601454108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07593804171147168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0759337485985394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07592945667522744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07592516594101933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07592087639539885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07591658803785016, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07591230086785758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07590801488490578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07590373008847971, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07589944647806465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0758951640531461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07589088281320991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07588660275774214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07588232388622926, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07587804619815793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07587376969301503, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07586949437028796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07586522022946413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0758609472700315, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07585667549147806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0758524048932923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07584813547496289, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07584386723597873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07583960017582922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07583533429400377, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07583106958999229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07582680606328482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07582254371337188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07581828253974403, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07581402254189229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07580976371930796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07580550607148245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0758012495979077, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07579699429807578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07579274017147904, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07578848721761018, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07578423543596215, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07577998482602814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07577573538730176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0757714871192767, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07576724002144716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07576299409330739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07575874933435212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07575450574407624, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07575026332197494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07574602206754373, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07574178198027841, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07573754305967499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07573330530522983, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07572906871643952, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07572483329280096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0757205990338113, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07571636593896805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07571213400776887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07570790323971183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07570367363429516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07569944519101754, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756952179093777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07569099178887481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07568676682900828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756825430292778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756783203891833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07567409890822506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756698785859036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07566565942171968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07566144141517439, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07565722456576905, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756530088730054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07564879433638519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0756445809554107, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07564036872958435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07563615765840884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07563194774138723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07562773897802287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07562353136781914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07561932491028, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07561511960490959, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07561091545121221, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07560671244869252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07560251059685559, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07559830989520643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07559411034325068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.075589911940494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07558571468644247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0755815185806024, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07557732362248039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07557312981158323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07556893714741811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07556474562949236, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0755605552573137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07555636603039007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0755521779482297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07554799101034104, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07554380521623291, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07553962056541427, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07553543705739448, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07553125469168313, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07552707346778999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07552289338522525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07551871444349925, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07551453664212276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07551035998060655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07550618445846197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07550201007520038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07549783683033358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07549366472337356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07548949375383263, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07548532392122334, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07548115522505844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07547698766485113, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07547282124011466, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07546865595036276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07546449179510929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07546032877386837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07545616688615447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07545200613148229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07544784650936681, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07544368801932325, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07543953066086712, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0754353744335142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07543121933678047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07542706537018233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07542291253323632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07541876082545922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0754146102463682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07541046079548061, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07540631247231412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07540216527638657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07539801920721617, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07539387426432134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07538973044722078, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0753855877554335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07538144618847867, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07537730574587582, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07537316642714466, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07536902823180529, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07536489115937797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07536075520938322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0753566203813419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07535248667477505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07534835408920404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07534422262415047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07534009227913624, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07533596305368344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07533183494731445, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.075327707959552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07532358208991896, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07531945733793853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07531533370313412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07531121118502948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07530708978314857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07530296949701562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07529885032615515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07529473227009188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0752906153283508, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07528649950045721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07528238478593667, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07527827118431495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07527415869511811, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0752700473178725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07526593705210465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07526182789734144, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07525771985310993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07525361291893747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07524950709435174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07524540237888053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07524129877205206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07523719627339465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07523309488243698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07522899459870797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07522489542173676, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07522079735105283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07521670038618583, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07521260452666563, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07520850977202258, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07520441612178702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0752003235754897, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07519623213266159, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07519214179283394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07518805255553818, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07518396442030609, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07517987738666966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07517579145416116, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0751717066223131, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07516762289065822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07516354025872957, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07515945872606032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07515537829218419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07515129895663482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07514722071894632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07514314357865297, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0751390675352893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07513499258839014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07513091873749052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07512684598212586, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07512277432183154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07511870375614359, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07511463428459793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07511056590673099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07510649862207926, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07510243243017964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07509836733056918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07509430332278529, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07509024040636547, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07508617858084768, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07508211784576992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07507805820067054, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07507399964508821, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07506994217856175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07506588580063027, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07506183051083314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07505777630870991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07505372319380052, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07504967116564502, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0750456202237838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07504157036775748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07503752159710692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07503347391137323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07502942731009772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07502538179282207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0750213373590881, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07501729400843801, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07501325174041405, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07500921055455886, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07500517045041538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0750011314275266, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07499709348543596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07499305662368698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07498902084182364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07498498613938992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07498095251593026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749769199709892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07497288850411163, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07496885811484262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07496482880272755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749608005673119, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749567734081416, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07495274732476272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749487223167216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07494469838356481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07494067552483914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07493665374009166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07493263302886971, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07492861339072086, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07492459482519287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07492057733183388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749165609101921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07491254555981608, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07490853128025464, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0749045180710568, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07490050593177187, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07489649486194931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07489248486113892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07488847592889074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07488446806475496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07488046126828218, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07487645553902299, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07487245087652854, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07486844728034994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07486444475003871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07486044328514659, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07485644288522549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07485244354982765, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07484844527850548, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0748444480708117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07484045192629922, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07483645684452121, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07483246282503114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0748284698673826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0748244779711295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.074820487135826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07481649736102644, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07481250864628551, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07480852099115803, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07480453439519907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07480054885796404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07479656437900849, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07479258095788825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07478859859415941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07478461728737826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0747806370371013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07477665784288537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0747726797042875, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07476870262086492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07476472659217516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07476075161777596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07475677769722529, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07475280483008138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07474883301590268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0747448622542479, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07474089254467596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07473692388674606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07473295628001764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07472898972405026, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07472502421840384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0747210597626386, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07471709635631481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07471313399899306, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07470917269023424, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0747052124295994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07470125321664985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07469729505094716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07469333793205311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07468938185952972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07468542683293918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0746814728518441, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07467751991580711, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07467356802439122, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07466961717715966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0746656673736758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07466171861350339, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07465777089620623, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07465382422134859, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07464987858849469, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07464593399720929, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07464199044705717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07463804793760341, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0746341064684133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07463016603905244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07462622664908657, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07462228829808173, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07461835098560414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07461441471122032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07461047947449695, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07460654527500099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07460261211229963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0745986799859603, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07459474889555058, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07459081884063841, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07458688982079191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07458296183557935, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0745790348845694, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07457510896733084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0745711840834326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07456726023244412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07456333741393478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07455941562747435, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0745554948726328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07455157514898032, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07454765645608737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07454373879352455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07453982216086277, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07453590655767317, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07453199198352706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07452807843799604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0745241659206519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07452025443106669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07451634396881268, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07451243453346237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07450852612458846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07450461874176394, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07450071238456196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07449680705255594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07449290274531953, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07448899946242658, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07448509720345123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0744811959679678, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07447729575555077, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.074473396565775, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07446949839821547, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07446560125244739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07446170512804631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07445781002458783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07445391594164792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07445002287880274, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0744461308356286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07444223981170214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0744383498066002, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07443446081989978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07443057285117828, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07442668590001303, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0744227999659819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07441891504866276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07441503114763384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07441114826247353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0744072663927605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07440338553807353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0743995056979918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07439562687209453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07439174905996135, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07438787226117193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0743839964753063, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07438012170194465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07437624794066743, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07437237519105529, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07436850345268907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07436463272514988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07436076300801912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07435689430087826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07435302660330914, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07434915991489363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07434529423521413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07434142956385295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07433756590039282, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0743337032444166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07432984159550739, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07432598095324856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07432212131722361, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07431826268701638, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07431440506221086, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07431054844239117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07430669282714188, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0743028382160476, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0742989846086932, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07429513200466384, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07429128040354478, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0742874298049216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07428358020838009, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0742797316135062, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07427588401988619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07427203742710642, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0742681918347536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07426434724241453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07426050364967639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07425666105612642, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07425281946135219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07424897886494142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07424513926648212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07424130066556242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07423746306177076, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07423362645469574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07422979084392622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07422595622905132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07422212260966023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07421828998534247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07421445835568774, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07421062772028607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0742067980787275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07420296943060248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0741991417755015, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07419531511301548, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07419148944273542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07418766476425248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07418384107715821, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07418001838104422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07417619667550245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07417237596012498, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07416855623450408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07416473749823242, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07416091975090264, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07415710299210775, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07415328722144093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07414947243849565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07414565864286544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07414184583414414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07413803401192587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07413422317580484, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07413041332537557, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0741266044602327, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07412279657997117, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0741189896841861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07411518377247288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07411137884442698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07410757489964422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07410377193772053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07409996995825217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07409616896083551, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07409236894506721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07408856991054408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07408477185686314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07408097478362172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07407717869041726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07407338357684742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07406958944251014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07406579628700355, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07406200410992599, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07405821291087587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07405442268945209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07405063344525356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07404684517787946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07404305788692916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0740392715720023, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07403548623269865, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07403170186861825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07402791847936133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07402413606452837, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07402035462371999, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07401657415653702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07401279466258062, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07400901614145206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07400523859275276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07400146201608451, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07399768641104924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07399391177724905, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07399013811428622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07398636542176343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07398259369928332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0739788229464489, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07397505316286343, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07397128434813016, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07396751650185275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07396374962363504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07395998371308102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0739562187697949, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07395245479338103, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07394869178344422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07394492973958919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07394116866142111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07393740854854514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07393364940056679, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07392989121709176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07392613399772589, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07392237774207532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07391862244974634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07391486812034546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07391111475347942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07390736234875508, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07390361090577965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07389986042416041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07389611090350495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07389236234342098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07388861474351653, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07388486810339966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0738811224226788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07387737770096255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0738736339378596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07386989113297907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07386614928593006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07386240839632198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07385866846376443, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07385492948786725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07385119146824044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0738474544044942, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07384371829623898, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07383998314308539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07383624894464429, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07383251570052665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07382878341034385, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07382505207370717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07382132169022831, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07381759225951919, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07381386378119184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07381013625485844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0738064096801316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07380268405662388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0737989593839481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07379523566171751, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07379151288954523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07378779106704482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07378407019382992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07378035026951445, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07377663129371245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07377291326603824, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07376919618610632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07376548005353137, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07376176486792825, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07375805062891215, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07375433733609826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0737506249891021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07374691358753947, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07374320313102618, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07373949361917836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07373578505161232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07373207742794458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07372837074779177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07372466501077088, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07372096021649893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07371725636459336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07371355345467157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07370985148635129, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07370615045925041, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07370245037298713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07369875122717966, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07369505302144655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07369135575540649, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07368765942867839, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07368396404088136, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0736802695916347, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07367657608055789, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07367288350727068, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07366919187139294, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07366550117254475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07366181141034642, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07365812258441849, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07365443469438157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07365074773985665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07364706172046476, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0736433766358272, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07363969248556543, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07363600926930118, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07363232698665631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07362864563725288, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07362496522071313, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07362128573665963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.073617607184715, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07361392956450206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07361025287564396, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0736065771177639, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07360290229048533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07359922839343196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07359555542622755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07359188338849618, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735882122798621, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07358454209994977, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07358087284838381, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07357720452478898, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07357353712879039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735698706600132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735662051180828, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735625405026249, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735588768132652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07355521404962977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07355155221134475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0735478912980365, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07354423130933169, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07354057224485704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07353691410423951, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07353325688710631, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07352960059308473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07352594522180238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.073522290772887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07351863724596648, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07351498464066894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07351133295662284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07350768219345655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07350403235079883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07350038342827861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07349673542552491, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0734930883421671, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07348944217783465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07348579693215716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07348215260476468, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.073478509195287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07347486670335464, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07347122512859786, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07346758447064736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07346394472913395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0734603059036887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07345666799394274, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07345303099952748, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07344939492007459, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07344575975521576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07344212550458304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07343849216780857, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07343485974452471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07343122823436393, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07342759763695908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07342396795194304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07342033917894893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07341671131761004, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07341308436755987, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07340945832843214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07340583319986074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07340220898147967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733985856729232, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07339496327382586, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733913417838222, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07338772120254707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07338410152963545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07338048276472262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07337686490744388, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733732479574349, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07336963191433138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733660167777693, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07336240254738487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07335878922281432, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07335517680369419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07335156528966123, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07334795468035234, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07334434497540455, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07334073617445525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07333712827714178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733335212831018, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07332991519197322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07332631000339401, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07332270571700239, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07331910233243674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07331549984933569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07331189826733793, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07330829758608252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07330469780520851, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0733010989243553, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07329750094316238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07329390386126944, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07329030767831633, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732867123939432, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732831180077903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07327952451949807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07327593192870703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07327234023505816, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07326874943819237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07326515953775091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07326157053337506, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07325798242470646, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732543952113868, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07325080889305806, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07324722346936224, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07324363893994178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07324005530443907, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732364725624968, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07323289071375778, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732293097578651, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732257296944619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732221505231917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07321857224369799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07321499485562454, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07321141835861536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0732078427523145, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07320426803636634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07320069421041535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07319712127410621, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0731935492270838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07318997806899323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07318640779947957, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0731828384181884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07317926992476524, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07317570231885587, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07317213560010621, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07316856976816248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07316500482267099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0731614407632782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07315787758963083, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07315431530137578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07315075389816003, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07314719337963087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07314363374543567, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07314007499522207, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07313651712863785, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07313296014533091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07312940404494943, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07312584882714168, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07312229449155622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07311874103784176, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.073115188465647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07311163677462114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0731080859644134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07310453603467308, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07310098698504977, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0730974388151933, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07309389152475357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07309034511338071, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.073086799580725, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07308325492643689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07307971115016713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07307616825156654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07307262623028607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07306908508597687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07306554481829043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0730620054268783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07305846691139213, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07305492927148387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07305139250680562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07304785661700958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07304432160174826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07304078746067427, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07303725419344043, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07303372179969964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07303019027910512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0730266596313102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07302312985596836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07301960095273333, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07301607292125888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07301254576119917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07300901947220838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07300549405394084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07300196950605115, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07299844582819412, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07299492302002462, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07299140108119782, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729878800113689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07298435981019336, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07298084047732682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729773220124251, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07297380441514419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729702876851402, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07296677182206954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07296325682558864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07295974269535428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07295622943102323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07295271703225255, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07294920549869947, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07294569483002138, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07294218502587581, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07293867608592051, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07293516800981334, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729316607972125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07292815444777619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729246489611628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07292114433703097, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0729176405750395, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07291413767484736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07291063563611357, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07290713445849759, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07290363414165876, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07290013468525683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0728966360889516, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07289313835240302, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07288964147527134, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07288614545721682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07288265029790007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07287915599698169, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07287566255412262, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0728721699689838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0728686782412265, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07286518737051216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0728616973565022, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07285820819885848, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07285471989724279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07285123245131726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07284774586074413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07284426012518579, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07284077524430486, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07283729121776403, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07283380804522628, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07283032572635477, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07282684426081265, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07282336364826346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07281988388837073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07281640498079833, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07281292692521017, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07280944972127038, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07280597336864328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07280249786699328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07279902321598512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07279554941528353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07279207646455352, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07278860436346021, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07278513311166894, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0727816627088452, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07277819315465468, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07277472444876316, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07277125659083669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0727677895805414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07276432341754362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07276085810150985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07275739363210687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07275393000900143, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07275046723186056, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07274700530035141, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0727435442141414, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07274008397289809, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07273662457628906, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07273316602398221, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07272970831564558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07272625145094738, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07272279542955594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07271934025113982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07271588591536769, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07271243242190846, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07270897977043114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07270552796060488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07270207699209917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07269862686458348, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07269517757772753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07269172913120114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07268828152467444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07268483475781758, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07268138883030095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07267794374179512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07267449949197073, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07267105608049867, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07266761350705006, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07266417177129607, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07266073087290799, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07265729081155747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07265385158691615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07265041319865594, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07264697564644891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07264353892996717, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07264010304888317, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07263666800286941, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07263323379159864, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07262980041474362, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07262636787197749, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07262293616297338, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07261950528740471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07261607524494497, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07261264603526787, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07260921765804726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07260579011295713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07260236339967173, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07259893751786539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07259551246721259, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07259208824738804, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07258866485806656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07258524229892321, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07258182056963311, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07257839966987166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07257497959931426, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07257156035763665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07256814194451466, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07256472435962427, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07256130760264161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.072557891673243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07255447657110496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07255106229590408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07254764884731722, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07254423622502133, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07254082442869353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07253741345801112, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07253400331265154, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07253059399229252, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07252718549661166, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07252377782528706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07252037097799673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07251696495441903, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0725135597542323, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07251015537711522, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07250675182274648, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.072503349090805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07249994718096993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07249654609292042, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07249314582633591, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07248974638089596, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07248634775628034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07248294995216888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07247955296824161, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07247615680417878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07247276145966079, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07246936693436812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07246597322798141, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07246258034018165, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07245918827064972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07245579701906686, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07245240658511434, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07244901696847375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07244562816882665, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07244224018585488, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07243885301924045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07243546666866543, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07243208113381218, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07242869641436307, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07242531251000074, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07242192942040798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07241854714526771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07241516568426304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0724117850370772, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07240840520339353, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07240502618289568, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07240164797526738, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07239827058019248, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07239489399735496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07239151822643912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07238814326712924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0723847691191099, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07238139578206576, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07237802325568155, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07237465153964241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07237128063363346, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07236791053733992, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07236454125044732, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07236117277264127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07235780510360755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07235443824303209, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07235107219060098, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0723477069460005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07234434250891698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07234097887903707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07233761605604747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07233425403963505, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07233089282948682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07232753242529003, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07232417282673194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07232081403350012, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07231745604528227, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07231409886176614, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07231074248263972, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07230738690759114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0723040321363087, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0723006781684808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07229732500379613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07229397264194336, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07229062108261142, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07228727032548937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07228392037026643, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.072280571216632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0722772228642756, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07227387531288691, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07227052856215575, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07226718261177216, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07226383746142626, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07226049311080836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07225714955960892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07225380680751856, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07225046485422808, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07224712369942836, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07224378334281045, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07224044378406565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07223710502288533, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07223376705896101, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0722304298919844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07222709352164734, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07222375794764183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07222042316966001, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07221708918739421, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07221375600053694, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0722104236087807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07220709201181835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0722037612093428, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07220043120104709, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07219710198662449, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0721937735657684, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0721904459381723, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07218711910352982, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07218379306153494, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07218046781188157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07217714335426387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07217381968837618, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07217049681391283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07216717473056852, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07216385343803801, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07216053293601617, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07215721322419802, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07215389430227888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.072150576169954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07214725882691896, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07214394227286938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07214062650750108, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07213731153051005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07213399734159244, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07213068394044446, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07212737132676249, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07212405950024324, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07212074846058329, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0721174382074796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07211412874062921, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0721108200597292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07210751216447694, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07210420505456995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07210089872970578, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07209759318958223, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07209428843389727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07209098446234892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07208768127463547, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07208437887045521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07208107724950674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07207777641148869, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07207447635609991, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07207117708303938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07206787859200615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07206458088269961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0720612839548191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07205798780806422, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07205469244213467, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07205139785673034, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07204810405155125, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07204481102629755, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07204151878066958, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07203822731436779, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07203493662709279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07203164671854534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07202835758842635, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07202506923643688, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07202178166227817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0720184948656515, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07201520884625844, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07201192360380064, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07200863913797988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07200535544849807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07200207253505733, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07199879039735986, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0719955090351082, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07199222844800471, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07198894863575217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0719856695980534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0719823913346113, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07197911384512908, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07197583712931, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07197256118685744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07196928601747499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0719660116208664, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07196273799673544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07195946514478627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07195619306472283, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07195292175624954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0719496512190708, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07194638145289127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07194311245741565, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07193984423234877, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07193657677739573, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07193331009226171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07193004417665194, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07192677903027196, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07192351465282737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07192025104402389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07191698820356747, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07191372613116413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07191046482652005, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07190720428934157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07190394451933521, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07190068551620754, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07189742727966539, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07189416980941564, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07189091310516532, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07188765716662171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07188440199349214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07188114758548411, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07187789394230513, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0718746410636632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07187138894926604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07186813759882191, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07186488701203887, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07186163718862536, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07185838812828987, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07185513983074102, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07185189229568764, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07184864552283861, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07184539951190304, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0718421542625902, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07183890977460937, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07183566604767008, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.071832423081482, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07182918087575493, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07182593943019878, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07182269874452363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07181945881843974, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07181621965165737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07181298124388716, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07180974359483967, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07180650670422574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07180327057175627, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07180003519714238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07179680058009523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07179356672032619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07179033361754682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07178710127146873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07178386968180368, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07178063884826363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717774087705606, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07177417944840689, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717709508815148, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07176772306959682, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07176449601236562, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07176126970953389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07175804416081465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07175481936592094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717515953245659, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07174837203646287, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717451495013254, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07174192771886707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07173870668880171, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717354864108431, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07173226688470538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717290481101027, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07172583008674938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07172261281435993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0717193962926489, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07171618052133105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07171296550012124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07170975122873458, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07170653770688616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07170332493429132, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07170011291066546, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07169690163572418, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07169369110918324, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716904813307585, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07168727230016592, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07168406401712164, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.071680856481342, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07167764969254335, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07167444365044225, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07167123835475549, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07166803380519984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07166483000149225, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07166162694334985, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716584246304899, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07165522306262984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07165202223948712, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716488221607794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07164562282622461, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07164242423554053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07163922638844535, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07163602928465726, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07163283292389465, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07162963730587595, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07162644243031978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07162324829694502, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716200549054705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07161686225561525, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07161367034709851, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07161047917963961, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07160728875295795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716040990667731, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0716009101208049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07159772191477314, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07159453444839788, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07159134772139916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07158816173349736, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07158497648441285, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07158179197386622, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07157860820157813, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07157542516726943, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07157224287066105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07156906131147413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07156588048942984, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0715627004042496, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07155952105565493, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07155634244336742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07155316456710889, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07154998742660128, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07154681102156654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07154363535172698, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07154046041680481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07153728621652261, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07153411275060284, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07153094001876832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0715277680207419, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07152459675624655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07152142622500537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07151825642674173, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07151508736117895, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07151191902804062, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07150875142705036, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07150558455793199, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07150241842040952, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07149925301420694, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07149608833904851, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07149292439465851, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07148976118076153, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07148659869708211, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07148343694334501, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07148027591927514, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07147711562459745, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07147395605903713, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07147079722231947, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07146763911416994, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07146448173431401, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07146132508247734, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07145816915838582, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07145501396176537, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07145185949234212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07144870574984225, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07144555273399208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07144240044451815, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07143924888114703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0714360980436055, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07143294793162047, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07142979854491888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07142664988322797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07142350194627492, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07142035473378724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07141720824549241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07141406248111819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0714109174403923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07140777312304271, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07140462952879753, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0714014866573849, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07139834450853326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07139520308197096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07139206237742673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07138892239462923, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07138578313330735, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07138264459319003, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0713795067740065, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07137636967548598, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07137323329735777, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07137009763935157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0713669627011969, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07136382848262356, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0713606949833615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07135756220314085, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07135443014169167, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07135129879874431, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07134816817402916, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07134503826727687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07134190907821814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07133878060658375, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07133565285210472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07133252581451206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07132939949353707, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07132627388891111, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0713231490003656, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0713200248276322, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07131690137044269, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07131377862852883, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07131065660162275, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07130753528945652, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07130441469176241, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07130129480827281, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07129817563872025, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07129505718283742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07129193944035703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07128882241101205, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07128570609453544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07128259049066049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07127947559912044, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07127636141964871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07127324795197881, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07127013519584456, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07126702315097962, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07126391181711805, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07126080119399389, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0712576912813413, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0712545820788946, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07125147358638835, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.071248365803557, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0712452587301354, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07124215236585832, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07123904671046066, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07123594176367769, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07123283752524447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07122973399489649, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07122663117236913, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07122352905739807, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07122042764971892, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07121732694906771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07121422695518033, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07121112766779299, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07120802908664181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07120493121146326, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07120183404199383, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07119873757797013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07119564181912888, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07119254676520705, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07118945241594156, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07118635877106963, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07118326583032845, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07118017359345544, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07117708206018812, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07117399123026417, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07117090110342127, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07116781167939741, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07116472295793053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07116163493875885, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0711585476216206, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0711554610062542, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07115237509239819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07114928987979116, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07114620536817197, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0711431215572795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07114003844685275, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0711369560366309, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07113387432635321, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07113079331575915, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07112771300458817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07112463339257996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07112155447947434, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07111847626501118, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07111539874893046, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07111232193097247, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07110924581087741, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07110617038838572, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07110309566323789, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07110002163517459, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07109694830393669, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07109387566926495, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07109080373090049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07108773248858447, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07108466194205816, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07108159209106295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0710785229353404, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07107545447463212, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07107238670867996, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07106931963722574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07106625326001151, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07106318757677949, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07106012258727185, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07105705829123105, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07105399468839955, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07105093177852007, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07104786956133538, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07104480803658834, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07104174720402198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0710386870633794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0710356276144039, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07103256885683884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07102951079042773, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07102645341491427, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07102339673004214, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0710203407355552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07101728543119752, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07101423081671321, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07101117689184647, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07100812365634172, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07100507110994339, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07100201925239613, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07099896808344473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07099591760283397, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07099286781030884, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07098981870561444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07098677028849605, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07098372255869899, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0709806755159687, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07097762916005076, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07097458349069097, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07097153850763507, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07096849421062909, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07096545059941904, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07096240767375114, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07095936543337175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0709563238780273, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07095328300746431, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0709502428214295, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07094720331966965, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07094416450193174, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07094112636796276, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07093808891750995, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0709350521503205, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07093201606614193, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07092898066472167, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07092594594580742, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07092291190914701, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07091987855448822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07091684588157918, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07091381389016792, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07091078258000279, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07090775195083208, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07090472200240432, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07090169273446814, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07089866414677229, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07089563623906558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07089260901109704, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708895824626157, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07088655659337084, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07088353140311177, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07088050689158794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07087748305854893, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708744599037444, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07087143742692426, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07086841562783834, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07086539450623673, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708623740618696, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708593542944873, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07085633520384012, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07085331678967871, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07085029905175358, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07084728198981566, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07084426560361574, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07084124989290481, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708382348574341, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07083522049695473, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0708322068112181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07082919379997571, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07082618146297917, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07082316979998014, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07082015881073049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07081714849498219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07081413885248727, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07081112988299797, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07080812158626655, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07080511396204545, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07080210701008724, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07079910073014452, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07079609512197013, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07079309018531692, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07079008591993796, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07078708232558632, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707840794020153, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07078107714897822, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707780755662286, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07077507465351998, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07077207441060619, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07076907483724094, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707660759331783, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07076307769817226, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07076008013197703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707570832343469, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707540870050363, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07075109144379976, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07074809655039198, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07074510232456771, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07074210876608178, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07073911587468924, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07073612365014519, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707331320922049, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707301412006237, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07072715097515703, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07072416141556055, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07072117252158988, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07071818429300089, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07071519672954948, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07071220983099175, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707092235970838, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0707062380275819, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07070325312224256, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07070026888082219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07069728530307744, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07069430238876499, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07069132013764184, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07068833854946487, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0706853576239912, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.070682377360978, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0706793977601826, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07067641882136246, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07067344054427512, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07067046292867823, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07066748597432954, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07066450968098702, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07066153404840861, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07065855907635243, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07065558476457683, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07065261111284003, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07064963812090053, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07064666578851693, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07064369411544794, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07064072310145238, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0706377527462891, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07063478304971721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07063181401149586, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07062884563138425, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07062587790914181, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07062291084452804, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07061994443730256, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07061697868722504, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07061401359405534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07061104915755344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07060808537747938, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07060512225359332, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07060215978565558, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07059919797342654, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07059623681666674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07059327631513679, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07059031646859737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0705873572768095, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.070584398739534, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07058144085653204, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07057848362756475, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07057552705239344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07057257113077962, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07056961586248474, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07056666124727048, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07056370728489855, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07056075397513091, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07055780131772943, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07055484931245634, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07055189795907374, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.070548947257344, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07054599720702955, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07054304780789292, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07054009905969674, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07053715096220387, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07053420351517711, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0705312567183795, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0705283105715741, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07052536507452416, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.070522420226993, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07051947602874409, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07051653247954093, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07051358957914718, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07051064732732672, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07050770572384328, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07050476476846096, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07050182446094382, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07049888480105616, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07049594578856225, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0704930074232265, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07049006970481352, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07048713263308798, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0704841962078146, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07048126042875831, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07047832529568408, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07047539080835706, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07047245696654245, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07046952377000552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07046659121851183, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07046365931182678, Trainning Accuracy: 99.16666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.07046072804971615, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07045779743194569, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07045486745828124, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07045193812848881, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07044900944233448, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07044608139958453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07044315400000523, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07044022724336296, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07043730112942438, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07043437565795604, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07043145082872472, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07042852664149737, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07042560309604085, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07042268019212233, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07041975792950896, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07041683630796813, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07041391532726715, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07041099498717364, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.0704080752874552, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07040515622787955, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07040223780821453, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07039932002822817, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07039640288768853, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07039348638636377, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07039057052402219, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07038765530043217, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07038474071536226, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07038182676858101, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07037891345985721, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07037600078895964, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07037308875565731, Trainning Accuracy: 99.16666666666667\n",
      "Training loss: 0.07037017735971922, Trainning Accuracy: 99.16666666666667\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "acc = []\n",
    "for i in range(max_iterations):\n",
    "    # pseudo code\n",
    "    feedforward_propagation()\n",
    "    loss_func()\n",
    "    gradient()\n",
    "    update theta, biase\n",
    "\n",
    "    losses.append(loss)\n",
    "    accuracy = compute_acc()\n",
    "    acc.append(accuracy)\n",
    "\n",
    "    print('Training loss: {}, Trainning Accuracy: {}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw your loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw your accuracy plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward_propagation()\n",
    "test_acc = compute_acc()\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve your network. \n",
    "# better follow the steps above.\n",
    "# Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
